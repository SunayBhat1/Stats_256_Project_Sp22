%% Generated by Sphinx.
\def\sphinxdocclass{jupyterBook}
\documentclass[letterpaper,10pt,english]{jupyterBook}
\ifdefined\pdfpxdimen
   \let\sphinxpxdimen\pdfpxdimen\else\newdimen\sphinxpxdimen
\fi \sphinxpxdimen=.75bp\relax
\ifdefined\pdfimageresolution
    \pdfimageresolution= \numexpr \dimexpr1in\relax/\sphinxpxdimen\relax
\fi
%% let collapsible pdf bookmarks panel have high depth per default
\PassOptionsToPackage{bookmarksdepth=5}{hyperref}
%% turn off hyperref patch of \index as sphinx.xdy xindy module takes care of
%% suitable \hyperpage mark-up, working around hyperref-xindy incompatibility
\PassOptionsToPackage{hyperindex=false}{hyperref}
%% memoir class requires extra handling
\makeatletter\@ifclassloaded{memoir}
{\ifdefined\memhyperindexfalse\memhyperindexfalse\fi}{}\makeatother

\PassOptionsToPackage{warn}{textcomp}

\catcode`^^^^00a0\active\protected\def^^^^00a0{\leavevmode\nobreak\ }
\usepackage{cmap}
\usepackage{fontspec}
\defaultfontfeatures[\rmfamily,\sffamily,\ttfamily]{}
\usepackage{amsmath,amssymb,amstext}
\usepackage{polyglossia}
\setmainlanguage{english}



\setmainfont{FreeSerif}[
  Extension      = .otf,
  UprightFont    = *,
  ItalicFont     = *Italic,
  BoldFont       = *Bold,
  BoldItalicFont = *BoldItalic
]
\setsansfont{FreeSans}[
  Extension      = .otf,
  UprightFont    = *,
  ItalicFont     = *Oblique,
  BoldFont       = *Bold,
  BoldItalicFont = *BoldOblique,
]
\setmonofont{FreeMono}[
  Extension      = .otf,
  UprightFont    = *,
  ItalicFont     = *Oblique,
  BoldFont       = *Bold,
  BoldItalicFont = *BoldOblique,
]



\usepackage[Bjarne]{fncychap}
\usepackage[,numfigreset=1,mathnumfig]{sphinx}

\fvset{fontsize=\small}
\usepackage{geometry}


% Include hyperref last.
\usepackage{hyperref}
% Fix anchor placement for figures with captions.
\usepackage{hypcap}% it must be loaded after hyperref.
% Set up styles of URL: it should be placed after hyperref.
\urlstyle{same}


\usepackage{sphinxmessages}



        % Start of preamble defined in sphinx-jupyterbook-latex %
         \usepackage[Latin,Greek]{ucharclasses}
        \usepackage{unicode-math}
        % fixing title of the toc
        \addto\captionsenglish{\renewcommand{\contentsname}{Contents}}
        \hypersetup{
            pdfencoding=auto,
            psdextra
        }
        % End of preamble defined in sphinx-jupyterbook-latex %
        

\title{Advanced Adjustment - An Introduction to Doubly Robust Methods}
\date{Jun 10, 2022}
\release{}
\author{S.\@{} Bhat, A.\@{} Chatterjee, L.\@{} Dahal, N.\@{} Hoffmann, A.\@{} Nanda}
\newcommand{\sphinxlogo}{\vbox{}}
\renewcommand{\releasename}{}
\makeindex
\begin{document}

\pagestyle{empty}
\sphinxmaketitle
\pagestyle{plain}
\sphinxtableofcontents
\pagestyle{normal}
\phantomsection\label{\detokenize{intro::doc}}


\begin{DUlineblock}{0em}
\item[] \sphinxstylestrong{\Large An Introduction to Doubly Robust Methods}
\end{DUlineblock}

\sphinxAtStartPar
\sphinxstylestrong{By Group 3:} Sunay Bhat, Ayush Chatterjee, Laxman Dahal, Nathan Hoffmann, Arya Nanda

\begin{DUlineblock}{0em}
\item[] \sphinxstylestrong{\sphinxstylestrong{Abstract}}
\end{DUlineblock}

\sphinxAtStartPar
In the following tutorial, we cover doubly robust estimation and key methods. We begin by detailing a brief history on the originals of doubly robust estimation and precursor methods. We then detail a precise formulation and demonstrate the mechanism by which doubly robust estimation is achieved. We further cover a few methods of interest including Augmented Inverse propensity Weighting (AIPW), Targeted maximum Likelihood Estimation (TMLE), and Double Machine Learning (DML). We use coding examples to illustrate the implementation and impacts of each of these methods in order to provide a clear and working understanding on doubly robust estimation methods.

\sphinxstepscope


\chapter{What does “doubly robust” mean?}
\label{\detokenize{Introduction and Conceptual Overview:what-does-doubly-robust-mean}}\label{\detokenize{Introduction and Conceptual Overview::doc}}
\sphinxAtStartPar
Doubly robust methods estimate two models:
\begin{itemize}
\item {} 
\sphinxAtStartPar
an \sphinxstyleemphasis{outcome model}

\end{itemize}
\begin{equation*}
\begin{split}\mu_d(X_i) = E(Y_i \mid D_i = d, X_i)\end{split}
\end{equation*}\begin{itemize}
\item {} 
\sphinxAtStartPar
and a \sphinxstyleemphasis{exposure model} (or treament model or propensity score):

\end{itemize}
\begin{equation*}
\begin{split}\pi(X_i) = E(D_i \mid X_i)\end{split}
\end{equation*}
\sphinxAtStartPar
where \(\mu_d(\cdot)\) is the model of control or treatment \(D_i = d=\{0, 1\}\), \(X_i\) is a vector of covariates for unit \(i = 1, \ldots, N\) for treatment (1) and control (0), \(Y_i\) is the outcome, and \(\pi(\cdot)\) is the exposure model. The covariates included in \(X_i\) can be different for the two models.

\sphinxAtStartPar
An estimator is called “doubly robust” if it achieves consistent estimation of the ATE (or whatever estimand we’re interested in) as long as \sphinxstyleemphasis{at least one} of these two models is consistently estimated. This means that the outcome model can be completely misspecified, but as long as the exposure model is correct, our estimation of the ATE will be consistent. This also means that the exposure model can be completely wrong, as along as the outcome model is correct.


\section{Origins of Doubly Robust Methods}
\label{\detokenize{Introduction and Conceptual Overview:origins-of-doubly-robust-methods}}
\sphinxAtStartPar
According to Bang and Robins (2005), doubly robust methods have their origins in missing data models. Robins, Rotnitzky, and Zhao (1994) and Rotnitzky, Robins, and Scharfstein (1998) developed augmented orthogonal inverse probability\sphinxhyphen{}weighted (AIPW) estimators in missing data models, and Scharfstein, Rotnitzky, and Robins (1999) showed that AIPW was doubly robust and extended to causal inference.

\sphinxAtStartPar
But Kang and Schafer (2007) argue that doubly robust methods are older. They cite work by Cassel, Särndal, and Wretman (1976), who proposed “generalized regression estimators” for population means from surveys where sampling weights must be estimated.

\sphinxAtStartPar
Arguably, doubly robust methods go back even further than this. The form of doubly robust methods is similar to residual\sphinxhyphen{}on\sphinxhyphen{}residual regression, which dates back to Frisch, Waugh, and Lovell (1933) famous FWL theorem:
\begin{equation*}
\begin{split}\beta_D = \frac{\text{Cov}(\tilde Y_i, \tilde D_i)}{\text{Var}(\tilde D_i)}\end{split}
\end{equation*}
\sphinxAtStartPar
where \(\tilde D_i\) is the residual part of \(D_i\) after regressing it on \(X_i\), and \(\tilde Y_i\) is the residual part of \(Y_i\) after regressing it on \(X_i\). This formulation writes the regression coefficient as composed of an outcome model (\(\tilde Y_i\)) and exposure model (\(\tilde D_i\)), the two models used in doubly robust estimators.

\sphinxAtStartPar
There are also links between doubly robust methods and matching with regression adjustment. This work goes back to at least Rubin (1973), who suggested that regression adjustment in matched data produces less biased estimates that either matching (exposure adjustment) or regression (outcome adjustment) do by themselves.


\section{Assumptions}
\label{\detokenize{Introduction and Conceptual Overview:assumptions}}
\sphinxAtStartPar
Most doubly robust methods require almost all of the standard assumptions necessary formost methods that depend on selection on observables. Although some doubly robust methods relax one or two of these, the six standard assumptions are:
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
Consistency

\item {} 
\sphinxAtStartPar
Positivity/overlap

\item {} 
\sphinxAtStartPar
One version of treatment

\item {} 
\sphinxAtStartPar
No interference

\item {} 
\sphinxAtStartPar
IID observations

\item {} 
\sphinxAtStartPar
Conditional ignorability: \(\{Y_{i0}, Y_{i1}\} \perp \!\!\! \perp D_i \mid X_i\)

\end{enumerate}

\sphinxAtStartPar
Special attention should be paid to Assumption 6: doubly robust methods will not work if we do not measure an important confounder that affects both treatment and exposure. But notably, the doubly robust methods covered in this tutorial make no functional form assumptions. Most use flexible machine learning algorithms to estimate both the outcome and exposure models, with regularization (often through cross\sphinxhyphen{}fitting) to avoid overfitting.

\sphinxAtStartPar
If these six assumptions are met, and we use the right estimator, we get double robustness: consistent estimation if either treatment or outcome model correct.


\section{A simple demonstration}
\label{\detokenize{Introduction and Conceptual Overview:a-simple-demonstration}}
\sphinxAtStartPar
To demonstrate double robustness, this section presents one of the simpler doubly robust estimators: augmented inverse probability weights (AIPW). The following is adapted from Chapter 12 of Matheus Facure Alves’s (2021) \sphinxstyleemphasis{\sphinxhref{https://matheusfacure.github.io/python-causality-handbook/landing-page.html}{Causal Inference for the Brave and True}}.

\sphinxAtStartPar
We can write this estimator as follows:
\begin{equation*}
\begin{split}\begin{aligned}
\widehat{ATE} = &\frac{1}{N} \sum_{i=1}^N \left( \frac{D_i(Y_i - \hat \mu_1 (X_i))}{\hat \pi (X_i)} + \hat \mu_1(X_i) \right) \\
&- \frac{1}{N} \sum_{i=1}^N \left( \frac{(1-D_i)(Y_i - \hat \mu_0 (X_i))}{1-\hat \pi(X_i)} + \hat \mu_0(X_i) \right)
\end{aligned}\end{split}
\end{equation*}
\sphinxAtStartPar
For each individual in the sample, this estimator calculates two quantities:
\begin{itemize}
\item {} 
\sphinxAtStartPar
The treated potential outcome

\end{itemize}
\begin{equation*}
\begin{split}\hat Y_{1i} = \frac{D_i(Y_i - \hat \mu_1 (X_i))}{\hat \pi (X_i)} + \hat \mu_1(X_i)\end{split}
\end{equation*}\begin{itemize}
\item {} 
\sphinxAtStartPar
The control potential outcome

\end{itemize}
\begin{equation*}
\begin{split}\hat Y_{0i} = \frac{(1-D_i)(Y_i - \hat \mu_0 (X_i))}{1-\hat \pi(X_i)} + \hat \mu_0(X_i)\end{split}
\end{equation*}
\sphinxAtStartPar
Let’s focus on the treated model:
\begin{equation*}
\begin{split}\hat Y_{1i} = \frac{D_i(Y_i - \hat \mu_1 (X_i))}{\hat \pi (X_i)} + \hat \mu_1(X_i)\end{split}
\end{equation*}
\sphinxAtStartPar
First, assume that the outcome model \(\mu_1(X_i)\) is \sphinxstyleemphasis{correctly} specified and the exposure model \(\pi(X_i)\) is \sphinxstyleemphasis{incorreclty} specified. Let’s also assume (for now) that we’re dealing with a treated unit, i.e. \(D_i = 1\). Then
\begin{equation*}
\begin{split}\hat \mu_1 (X_i) = Y_i\end{split}
\end{equation*}
\sphinxAtStartPar
and hence
\begin{equation*}
\begin{split}\hat Y_{1i} = \frac{D_i(0)}{\hat \pi (X_i)} + \hat \mu_1(X_i) = \hat \mu_1(X_i).\end{split}
\end{equation*}
\sphinxAtStartPar
So the model relies \sphinxstyleemphasis{only} on the outcome model! The incorrectly specified exposure model completely disappears from the equation. If we’re dealing with a control unit (\(D_i=0\)), we get the same result:
\begin{equation*}
\begin{split}\hat Y_{1i} = \frac{0(Y_i - \hat \mu_1 (X_i))}{\hat \pi (X_i)} + \hat \mu_1(X_i) = \hat \mu_1(X_i).\end{split}
\end{equation*}
\sphinxAtStartPar
Now, what if the \sphinxstyleemphasis{exposure} model \(\pi(X_i)\) is correctly specified and the outcome model \(\mu_1(X)\) is incorrect? First, we rewrite the estimator for the treated outcome:
\begin{equation*}
\begin{split}\begin{aligned}
\hat Y_{1i}& = \frac{D_i(Y_i - \hat \mu_1 (X_i))}{\hat \pi (X_i)} + \hat \mu_1(X_i) \\
&= \frac{D_iY_i}{\hat \pi (X_i)} - \frac{D_i\hat \mu_1 (X_i)}{\hat \pi (X_i)} + \frac{\hat \pi (X_i)\hat \mu_1(X_i)}{\hat \pi (X_i)} \\
& = \frac{D_iY_i}{\hat \pi (X_i)} - \left( \frac{D_i - \hat \pi(X_i)}{\hat \pi (X_i)}\right) \hat \mu_1(X_i). &&(*)
\end{aligned}\end{split}
\end{equation*}
\sphinxAtStartPar
Since the exposure model is correclty specified, we have \(D_i = \hat \pi(X_i)\) on average, so
\begin{equation*}
\begin{split}E[D_i - \hat \pi(X_i)] = 0.\end{split}
\end{equation*}
\sphinxAtStartPar
This means that the second term in equation \((*)\) is 0, so
\begin{equation*}
\begin{split}E[\hat Y_{1i}]= E \left [ \frac{D_iY_i}{\hat \pi (X_i)}\right].\end{split}
\end{equation*}
\sphinxAtStartPar
This shows that when the exposure model is correct, then the estimator depends \sphinxstyleemphasis{only} on the exposure model. We can make similar arguments for the control model \(\hat Y_{0i}\).

\sphinxAtStartPar
This demonstration shows that this estimator achieves double robustness: the estimator is robust to misspecification of either the exposure or the outcome model (but not both!).


\subsection{References}
\label{\detokenize{Introduction and Conceptual Overview:references}}
\sphinxAtStartPar
Bang, H., \& Robins, J. M. (2005). Doubly Robust Estimation in Missing Data and Causal Inference Models. \sphinxstyleemphasis{Biometrics}, 61(4), 962–973. \sphinxurl{https://doi.org/10.1111/j.1541-0420.2005.00377.x}

\sphinxAtStartPar
CASSEL, C. M., SÄRNDAL, C. E., \& WRETMAN, J. H. (1976). Some results on generalized difference estimation and generalized regression estimation for finite populations. \sphinxstyleemphasis{Biometrika}, 63(3), 615–620. \sphinxurl{https://doi.org/10.1093/biomet/63.3.615}

\sphinxAtStartPar
Frisch, R., \& Waugh, F. V. (1933). Partial Time Regressions as Compared with Individual Trends. \sphinxstyleemphasis{Econometrica}, 1(4), 387–401. \sphinxurl{https://doi.org/10.2307/1907330}

\sphinxAtStartPar
Kang, J. D. Y., \& Schafer, J. L. (2007). Demystifying Double Robustness: A Comparison of Alternative Strategies for Estimating a Population Mean from Incomplete Data. \sphinxstyleemphasis{Statistical Science}, 22(4), 523–539. \sphinxurl{https://doi.org/10.1214/07-STS227}

\sphinxAtStartPar
Robins, J. M., Rotnitzky, A., \& Zhao, L. P. (1994). Estimation of Regression Coefficients When Some Regressors are not Always Observed. \sphinxstyleemphasis{Journal of the American Statistical Association}, 89(427), 846–866. \sphinxurl{https://doi.org/10.1080/01621459.1994.10476818}

\sphinxAtStartPar
Rotnitzky, A., Robins, J. M., \& Scharfstein, D. O. (1998). Semiparametric Regression for Repeated Outcomes with Nonignorable Nonresponse. \sphinxstyleemphasis{Journal of the American Statistical Association}, 93(444), 1321–1339. \sphinxurl{https://doi.org/10.2307/2670049}

\sphinxAtStartPar
Rubin, D. B. (1973). The Use of Matched Sampling and Regression Adjustment to Remove Bias in Observational Studies. \sphinxstyleemphasis{Biometrics}, 29(1), 185–203. \sphinxurl{https://doi.org/10.2307/2529685}

\sphinxAtStartPar
Scharfstein, D. O., Rotnitzky, A., \& Robins, J. M. (1999). Adjusting for Nonignorable Drop\sphinxhyphen{}Out Using Semiparametric Nonresponse Models. \sphinxstyleemphasis{Journal of the American Statistical Association}, 94(448), 1096–1120. \sphinxurl{https://doi.org/10.1080/01621459.1999.10473862}

\sphinxstepscope


\chapter{Doubly Robust Methods}
\label{\detokenize{DR_Methods:doubly-robust-methods}}\label{\detokenize{DR_Methods::doc}}

\section{AIPW}
\label{\detokenize{DR_Methods:aipw}}

\subsection{Background}
\label{\detokenize{DR_Methods:background}}
\sphinxAtStartPar
Augmented Inverse Propensity Weighting (AIPW) is a modification of standard Inverse Propensity Weighting to achieve double robustness. We first consider basic IPW, which considers a sample weight, or propensity score \(\hat \pi (X_i)\), in the model.
\begin{equation*}
\begin{split}\widehat{ATE}_{IPW} = \frac{1}{N} \sum_{i=1}^N \left[\frac{D_iY_i}{\hat \pi (X_i)} - \frac{(1-D_i)Y_i}{1-\hat \pi (X_i)}\right]\end{split}
\end{equation*}
\sphinxAtStartPar
The augmenteed IPW, AIPW, as presetned by Glynn and Quinn, 2009 includes the outcome model in such a way that ensures doubly\sphinxhyphen{}robust estimation. This equations below reqrites the AIPW formulation such the basic IPW is seen clearly first along with the ourcome model augmentation.
\begin{equation*}
\begin{split}\widehat{ATE}_{AIPW} = \frac{1}{N} \sum_{i=1}^N \left(\left[\frac{D_iY_i}{\hat \pi (X_i)} - \frac{(1-D_i)Y_i}{1-\hat \pi (X_i)}\right]-\frac{(X_i - \hat \pi (X_i))Y_i }{\pi (X_i)(1-\pi (X_i))} \right) [(1-\hat \pi (X_i))\hat{\mathbb{E}}(Y_i|D_i=1,X_i) + + \hat{\pi} (X_i) \hat{\mathbb{E}}(Y_i|D_i=0,X_i)]\end{split}
\end{equation*}
\begin{sphinxuseclass}{cell}
\begin{sphinxuseclass}{tag_hide-input}
\end{sphinxuseclass}
\end{sphinxuseclass}

\subsection{Dataset}
\label{\detokenize{DR_Methods:dataset}}
\sphinxAtStartPar
We will use a simulated dataset based on The National Study of Learning Mindsets. This was a randomized study conducted in U.S. public high schools, the purpose of which was to evaluate the impact of a nudge\sphinxhyphen{}like, optional intervention designed to instill students with a growth mindset. The study includes measured outcomes via an achievement score, a binary treatment of a growth mindset educational intervention, and 11 other potential confounding factors that could be parents of both the treatment and outcome. The first 5 rows of the dataset are shows in the table below.

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{df\PYGZus{}mindset} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{read\PYGZus{}csv}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{learning\PYGZus{}mindset.csv}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{c+c1}{\PYGZsh{} print(df\PYGZus{}mindset.info())}
\PYG{n}{df\PYGZus{}mindset}\PYG{o}{.}\PYG{n}{head}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
   schoolid  intervention  achievement\PYGZus{}score  success\PYGZus{}expect  ethnicity  \PYGZbs{}
0        76             1              0.277               6          4   
1        76             1             \PYGZhy{}0.450               4         12   
2        76             1              0.770               6          4   
3        76             1             \PYGZhy{}0.122               6          4   
4        76             1              1.526               6          4   

   gender  frst\PYGZus{}in\PYGZus{}family  school\PYGZus{}urbanicity  school\PYGZus{}mindset  \PYGZbs{}
0       2               1                  4           0.335   
1       2               1                  4           0.335   
2       2               0                  4           0.335   
3       2               0                  4           0.335   
4       1               0                  4           0.335   

   school\PYGZus{}achievement  school\PYGZus{}ethnic\PYGZus{}minority  school\PYGZus{}poverty  school\PYGZus{}size  
0               0.649                  \PYGZhy{}1.311           0.224       \PYGZhy{}0.427  
1               0.649                  \PYGZhy{}1.311           0.224       \PYGZhy{}0.427  
2               0.649                  \PYGZhy{}1.311           0.224       \PYGZhy{}0.427  
3               0.649                  \PYGZhy{}1.311           0.224       \PYGZhy{}0.427  
4               0.649                  \PYGZhy{}1.311           0.224       \PYGZhy{}0.427  
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\begin{sphinxuseclass}{cell}
\begin{sphinxuseclass}{tag_hide-input}
\end{sphinxuseclass}
\end{sphinxuseclass}

\subsection{Understanding the data and potential bias}
\label{\detokenize{DR_Methods:understanding-the-data-and-potential-bias}}
\sphinxAtStartPar
We begin by visualizing the achievement scores of treated and untreated cohorts with no control or consideration for the other variables. It is clear form the plot below there is an impact of the treatment as the average of the treated group’s achievement scores is clearly higher. But we can intuit a positive bias in this measurement. We should note again the intervention was an option to take a growth mindset course. So although the option was offered in a random fashion, \sphinxstyleemphasis{it is highly likely students who opt\sphinxhyphen{}in to the treatment are likely to have the features to provide higher achievement scores regardless}. Thus, we might hypothesize controlling for this bias would decrease the ATE from the \sphinxstyleemphasis{naive ATE} (meaning no adjustment or simple difference of means of the treated and untreated groups).

\begin{sphinxuseclass}{cell}
\begin{sphinxuseclass}{tag_hide-input}\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{DR_Methods_6_0}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\end{sphinxuseclass}

\subsection{Method Implementations}
\label{\detokenize{DR_Methods:method-implementations}}
\sphinxAtStartPar
The following code block implements the naive ATE, the standard IPW, and finally the AIPW methods as python functions. Note that propensity score, or the exposure model, is constructed as a \sphinxstyleemphasis{Logistic Regression problem}, and the outcome model is generated as a \sphinxstyleemphasis{Linear Regression problem}.

\sphinxAtStartPar
We do this to allow us to readily run many iterations of each method. We will use a bootstrap subsample method, where we will sample the full dataset with replacement 100 times. This will allow us to generate a distribution of ATEs with an empirical standard deviation. Thus we can report our results comparing each of the three methods using various exposure and outcome models with 95\% confidence intervals as well.

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{} Define Estimation methods \PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}}

\PYG{c+c1}{\PYGZsh{}\PYGZsh{}\PYGZsh{} Linear Regression T on Y}
\PYG{k}{def} \PYG{n+nf}{naive\PYGZus{}ATE}\PYG{p}{(}\PYG{n}{df}\PYG{p}{,} \PYG{n}{T}\PYG{p}{,} \PYG{n}{Y}\PYG{p}{)}\PYG{p}{:}
    \PYG{k}{return} \PYG{n}{df}\PYG{p}{[}\PYG{n}{Y}\PYG{p}{]}\PYG{p}{[}\PYG{n}{df\PYGZus{}categ}\PYG{p}{[}\PYG{n}{T}\PYG{p}{]} \PYG{o}{==} \PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{p}{)} \PYG{o}{\PYGZhy{}} \PYG{n}{df\PYGZus{}categ}\PYG{p}{[}\PYG{n}{Y}\PYG{p}{]}\PYG{p}{[}\PYG{n}{df\PYGZus{}categ}\PYG{p}{[}\PYG{n}{T}\PYG{p}{]} \PYG{o}{==} \PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{}\PYGZsh{}\PYGZsh{} IPW}
\PYG{k}{def} \PYG{n+nf}{IPW}\PYG{p}{(}\PYG{n}{df}\PYG{p}{,} \PYG{n}{X}\PYG{p}{,} \PYG{n}{T}\PYG{p}{,} \PYG{n}{Y}\PYG{p}{,}\PYG{n}{true\PYGZus{}ps} \PYG{o}{=} \PYG{k+kc}{True}\PYG{p}{)}\PYG{p}{:}

    \PYG{k}{if} \PYG{n}{true\PYGZus{}ps}\PYG{p}{:}
        \PYG{n}{p\PYGZus{}scores} \PYG{o}{=} \PYG{n}{LogisticRegression}\PYG{p}{(}\PYG{n}{C}\PYG{o}{=}\PYG{l+m+mf}{1e6}\PYG{p}{,} \PYG{n}{max\PYGZus{}iter}\PYG{o}{=}\PYG{l+m+mi}{1000}\PYG{p}{)}\PYG{o}{.}\PYG{n}{fit}\PYG{p}{(}\PYG{n}{df}\PYG{p}{[}\PYG{n}{X}\PYG{p}{]}\PYG{p}{,} \PYG{n}{df}\PYG{p}{[}\PYG{n}{T}\PYG{p}{]}\PYG{p}{)}\PYG{o}{.}\PYG{n}{predict\PYGZus{}proba}\PYG{p}{(}\PYG{n}{df}\PYG{p}{[}\PYG{n}{X}\PYG{p}{]}\PYG{p}{)}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{]}
    \PYG{k}{else}\PYG{p}{:}
        \PYG{n}{p\PYGZus{}scores} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{uniform}\PYG{p}{(}\PYG{l+m+mf}{0.1}\PYG{p}{,} \PYG{l+m+mf}{0.9}\PYG{p}{,} \PYG{n}{df}\PYG{o}{.}\PYG{n}{shape}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{)}

    \PYG{n}{df\PYGZus{}ps} \PYG{o}{=} \PYG{n}{df}\PYG{o}{.}\PYG{n}{assign}\PYG{p}{(}\PYG{n}{propensity\PYGZus{}score}\PYG{o}{=}\PYG{n}{p\PYGZus{}scores}\PYG{p}{)}

    \PYG{n}{weight} \PYG{o}{=} \PYG{p}{(}\PYG{p}{(}\PYG{n}{df\PYGZus{}ps}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{intervention}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}\PYG{o}{\PYGZhy{}}\PYG{n}{df\PYGZus{}ps}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{propensity\PYGZus{}score}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}\PYG{p}{)} \PYG{o}{/} \PYG{p}{(}\PYG{n}{df\PYGZus{}ps}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{propensity\PYGZus{}score}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}\PYG{o}{*}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{o}{\PYGZhy{}}\PYG{n}{df\PYGZus{}ps}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{propensity\PYGZus{}score}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}\PYG{p}{)}\PYG{p}{)}\PYG{p}{)}

    \PYG{n}{weight\PYGZus{}t} \PYG{o}{=} \PYG{l+m+mi}{1}\PYG{o}{/}\PYG{n}{df\PYGZus{}ps}\PYG{o}{.}\PYG{n}{query}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{intervention==1}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{propensity\PYGZus{}score}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}
    \PYG{n}{weight\PYGZus{}nt} \PYG{o}{=} \PYG{l+m+mi}{1}\PYG{o}{/}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{o}{\PYGZhy{}}\PYG{n}{df\PYGZus{}ps}\PYG{o}{.}\PYG{n}{query}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{intervention==0}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{propensity\PYGZus{}score}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}\PYG{p}{)}

    \PYG{n}{y1} \PYG{o}{=} \PYG{n+nb}{sum}\PYG{p}{(}\PYG{n}{df\PYGZus{}ps}\PYG{o}{.}\PYG{n}{query}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{intervention==1}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{achievement\PYGZus{}score}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}\PYG{o}{*}\PYG{n}{weight}\PYG{p}{)} \PYG{o}{/} \PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{df\PYGZus{}ps}\PYG{p}{)}
    \PYG{n}{y0} \PYG{o}{=} \PYG{n+nb}{sum}\PYG{p}{(}\PYG{n}{df\PYGZus{}ps}\PYG{o}{.}\PYG{n}{query}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{intervention==0}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{achievement\PYGZus{}score}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}\PYG{o}{*}\PYG{n}{weight\PYGZus{}nt}\PYG{p}{)} \PYG{o}{/} \PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{df\PYGZus{}ps}\PYG{p}{)}

    \PYG{k}{return} \PYG{n}{np}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{n}{weight} \PYG{o}{*} \PYG{n}{df\PYGZus{}ps}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{achievement\PYGZus{}score}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}\PYG{p}{)}\PYG{p}{,} \PYG{n}{p\PYGZus{}scores}\PYG{p}{,} \PYG{n}{df\PYGZus{}ps}

\PYG{c+c1}{\PYGZsh{}\PYGZsh{}\PYGZsh{} AIPW}
\PYG{k}{def} \PYG{n+nf}{AIPW}\PYG{p}{(}\PYG{n}{df}\PYG{p}{,} \PYG{n}{X}\PYG{p}{,} \PYG{n}{T}\PYG{p}{,} \PYG{n}{Y}\PYG{p}{,}\PYG{n}{true\PYGZus{}ps} \PYG{o}{=} \PYG{k+kc}{True}\PYG{p}{,}\PYG{n}{true\PYGZus{}mus} \PYG{o}{=} \PYG{k+kc}{True}\PYG{p}{)}\PYG{p}{:}
    \PYG{k}{if} \PYG{n}{true\PYGZus{}ps}\PYG{p}{:}
        \PYG{n}{p\PYGZus{}scores} \PYG{o}{=} \PYG{n}{LogisticRegression}\PYG{p}{(}\PYG{n}{C}\PYG{o}{=}\PYG{l+m+mf}{1e6}\PYG{p}{,}\PYG{n}{max\PYGZus{}iter}\PYG{o}{=}\PYG{l+m+mi}{1000}\PYG{p}{)}\PYG{o}{.}\PYG{n}{fit}\PYG{p}{(}\PYG{n}{df}\PYG{p}{[}\PYG{n}{X}\PYG{p}{]}\PYG{p}{,} \PYG{n}{df}\PYG{p}{[}\PYG{n}{T}\PYG{p}{]}\PYG{p}{)}\PYG{o}{.}\PYG{n}{predict\PYGZus{}proba}\PYG{p}{(}\PYG{n}{df}\PYG{p}{[}\PYG{n}{X}\PYG{p}{]}\PYG{p}{)}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{]}
    \PYG{k}{else}\PYG{p}{:}
        \PYG{n}{p\PYGZus{}scores} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{uniform}\PYG{p}{(}\PYG{l+m+mf}{0.1}\PYG{p}{,} \PYG{l+m+mf}{0.9}\PYG{p}{,} \PYG{n}{df}\PYG{o}{.}\PYG{n}{shape}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{)}

    \PYG{k}{if} \PYG{n}{true\PYGZus{}mus}\PYG{p}{:}
        \PYG{n}{mu0} \PYG{o}{=} \PYG{n}{LinearRegression}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n}{fit}\PYG{p}{(}\PYG{n}{df}\PYG{o}{.}\PYG{n}{query}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+si}{\PYGZob{}}\PYG{n}{T}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{==0}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}\PYG{p}{[}\PYG{n}{X}\PYG{p}{]}\PYG{p}{,} \PYG{n}{df}\PYG{o}{.}\PYG{n}{query}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+si}{\PYGZob{}}\PYG{n}{T}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{==0}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}\PYG{p}{[}\PYG{n}{Y}\PYG{p}{]}\PYG{p}{)}\PYG{o}{.}\PYG{n}{predict}\PYG{p}{(}\PYG{n}{df}\PYG{p}{[}\PYG{n}{X}\PYG{p}{]}\PYG{p}{)}
        \PYG{n}{mu1} \PYG{o}{=} \PYG{n}{LinearRegression}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n}{fit}\PYG{p}{(}\PYG{n}{df}\PYG{o}{.}\PYG{n}{query}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+si}{\PYGZob{}}\PYG{n}{T}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{==1}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}\PYG{p}{[}\PYG{n}{X}\PYG{p}{]}\PYG{p}{,} \PYG{n}{df}\PYG{o}{.}\PYG{n}{query}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+si}{\PYGZob{}}\PYG{n}{T}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{==1}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}\PYG{p}{[}\PYG{n}{Y}\PYG{p}{]}\PYG{p}{)}\PYG{o}{.}\PYG{n}{predict}\PYG{p}{(}\PYG{n}{df}\PYG{p}{[}\PYG{n}{X}\PYG{p}{]}\PYG{p}{)}
    \PYG{k}{else}\PYG{p}{:}
        \PYG{n}{mu0} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{uniform}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{df}\PYG{o}{.}\PYG{n}{shape}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{)}
        \PYG{n}{mu1} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{uniform}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{df}\PYG{o}{.}\PYG{n}{shape}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{)}

    \PYG{k}{return} \PYG{p}{(}
        \PYG{n}{np}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{n}{df}\PYG{p}{[}\PYG{n}{T}\PYG{p}{]}\PYG{o}{*}\PYG{p}{(}\PYG{n}{df}\PYG{p}{[}\PYG{n}{Y}\PYG{p}{]} \PYG{o}{\PYGZhy{}} \PYG{n}{mu1}\PYG{p}{)}\PYG{o}{/}\PYG{n}{p\PYGZus{}scores} \PYG{o}{+} \PYG{n}{mu1}\PYG{p}{)} \PYG{o}{\PYGZhy{}}
        \PYG{n}{np}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{o}{\PYGZhy{}}\PYG{n}{df}\PYG{p}{[}\PYG{n}{T}\PYG{p}{]}\PYG{p}{)}\PYG{o}{*}\PYG{p}{(}\PYG{n}{df}\PYG{p}{[}\PYG{n}{Y}\PYG{p}{]} \PYG{o}{\PYGZhy{}} \PYG{n}{mu0}\PYG{p}{)}\PYG{o}{/}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{o}{\PYGZhy{}}\PYG{n}{p\PYGZus{}scores}\PYG{p}{)} \PYG{o}{+} \PYG{n}{mu0}\PYG{p}{)}
    \PYG{p}{)}\PYG{p}{,} \PYG{n}{p\PYGZus{}scores}\PYG{p}{,} \PYG{n}{mu0}\PYG{p}{,} \PYG{n}{mu1}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}

\subsection{Experiments and Results}
\label{\detokenize{DR_Methods:experiments-and-results}}
\sphinxAtStartPar
The following  block shows our bootstrap sampling method results displayed (100 iterations for the full dataset). In this initial experiment, we correctly specify both the exposure and outcome models. The results are displayed in the plot and table below.

\begin{sphinxuseclass}{cell}
\begin{sphinxuseclass}{tag_hide-input}\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
       Mean ATE  Std Dev  [.025  .975]
AIPW      0.386    0.017  0.351  0.420
IPW       0.386    0.016  0.352  0.419
Naive     0.471    0.015  0.442  0.497
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{DR_Methods_10_1}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\end{sphinxuseclass}
\sphinxAtStartPar
From the results it is clear both IPW and AIPW account for a positive bias we hypothesized. They estimate the ATE at \textasciitilde{}\(0.39\), up from the naive ATE estimate of \textasciitilde{}\(0.47\). We also note the IPW and AIPW methods agree with very close estimates and with very similar 95\% confidence intervals. This is unsurprising considering the exposure model is correctly specified using logistic regression for both methods.

\sphinxAtStartPar
Now that we have propensity scores, we can also perform a quick positivity check visualizing the distribution of our propensity scores to ensure we meet the positivity/overlap assumption which the plot below demonstrates.

\begin{sphinxuseclass}{cell}
\begin{sphinxuseclass}{tag_hide-input}\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{DR_Methods_12_0}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\end{sphinxuseclass}
\sphinxAtStartPar
In the second experiment, we specify a bad exposure model. Instead of using logistic regression, we simply sample a uniform random distribution:
\begin{equation*}
\begin{split} \hat \pi (X_i) \sim U(0.1,0.9) \end{split}
\end{equation*}
\sphinxAtStartPar
As we can see from the results below, the AIPE method is effectively stable, estimating a slightly lower ATE of about \textasciitilde{}\(0.38\). The standard deviation also increases slightly. On the other hand, the IPW method does far worse than the naive method, which again makes sense as we are feeding it random noise for the propensity scores. This is the first example of a doubly robust method showing how, since the outcome model is correctly specified, the estimation is still robust even to random noise for the exposure model.

\begin{sphinxuseclass}{cell}
\begin{sphinxuseclass}{tag_hide-input}\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
       Mean ATE  Std Dev  [.025  .975]
AIPW      0.391    0.022  0.354  0.435
IPW       0.580    0.031  0.523  0.648
Naive     0.473    0.018  0.443  0.511
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{DR_Methods_14_1}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\end{sphinxuseclass}
\sphinxAtStartPar
In the third experiment, we now investigate the impact of a bad outcome model. We again sample from a uniform distribution to obtain the incorrect outcome data:
\begin{equation*}
\begin{split}\mu_d(X_i) \sim U(0,1) \end{split}
\end{equation*}
\sphinxAtStartPar
Here once again see the AIPW and IPW methods both agree and estimate \textasciitilde{}\(0.39\). AIPW again shows the doubly robust property against the completely random outcome model, while IPW is unimpacted since the exposure model is correct. Both hence perform similarly to the original experiment.

\begin{sphinxuseclass}{cell}
\begin{sphinxuseclass}{tag_hide-input}\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
       Mean ATE  Std Dev  [.025  .975]
AIPW      0.385    0.018  0.348  0.417
IPW       0.386    0.017  0.351  0.414
Naive     0.472    0.020  0.441  0.509
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{DR_Methods_16_1}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\end{sphinxuseclass}
\sphinxAtStartPar
In the final experiment, we show the impact of a bad outcome and exposure model:
\begin{equation*}
\begin{split}\mu_d(X_i) \sim U(0,1),  \hat \pi (X_i) \sim U(0.1,0.9) \end{split}
\end{equation*}
\sphinxAtStartPar
In this experiment, we see that AIPW performs very poorly, vastly over\sphinxhyphen{}estimating the ATE. In this instance, either naive or IPW would perform better, although the naive without any consideration for random models does best.

\begin{sphinxuseclass}{cell}
\begin{sphinxuseclass}{tag_hide-input}\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
       Mean ATE  Std Dev  [.025  .975]
AIPW      1.044    0.041  0.958  1.123
IPW       0.571    0.034  0.510  0.642
Naive     0.471    0.017  0.441  0.505
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{DR_Methods_18_1}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\end{sphinxuseclass}

\subsection{Concluding Thoughts for AIPE}
\label{\detokenize{DR_Methods:concluding-thoughts-for-aipe}}
\sphinxAtStartPar
We clearly demonstrate AIPW’s doubly robust properties using the simulated National Mindset dataset. But it is important to note, our ‘incorrect’ models were uniform random which would be about as poor as one could imagine. In reality, misspecified models contain more subtle biases or noise, and thus there is a whole host of literature investigating the sensitivity of doubly robust methods to various types and degrees of misspecification. For instance in the example where both models were incorrect, one could imagine scenarios where model misspecifications cancel out, and actually produce a relatively accurate ATE estimate. It is an area of active research on when doubly robust methods should be used when there might be uncertainty on both models.


\section{TMLE}
\label{\detokenize{DR_Methods:tmle}}

\subsection{Background and Setup}
\label{\detokenize{DR_Methods:background-and-setup}}
\sphinxAtStartPar
Targeted Maximum Likelihood Estimation (TMLE) is a semi\sphinxhyphen{}parametric method with minimal assumptions on the underlying data distribution demonstrated by Van der Laan \& Rubin in 2006. We will briefly walk through the steps of a TMLE estimation algorithm on the same data without diving too deep into the formulation.

\begin{sphinxuseclass}{cell}
\begin{sphinxuseclass}{tag_hide-input}
\end{sphinxuseclass}
\end{sphinxuseclass}
\sphinxAtStartPar
The TMLE algorithm begins by first estimating a model by training and predicting a super learning ensemble of algorithms. In the hidden code block below, we do so using 9 models from pre\sphinxhyphen{}built libraries. We report the root mean squared errors for all algorithms, demonstrating the super learner ensemble performs best.

\begin{sphinxuseclass}{cell}
\begin{sphinxuseclass}{tag_hide-input}\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
Train (5195, 31) (5195,) Test (5196, 31) (5196,)
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Meta  (5195, 9) (5195,)
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
LinearRegression: RMSE 0.832
ElasticNet: RMSE 0.998
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
SVR: RMSE 0.853
DecisionTreeRegressor: RMSE 1.035
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
KNeighborsRegressor: RMSE 0.890
AdaBoostRegressor: RMSE 0.831
BaggingRegressor: RMSE 0.894
RandomForestRegressor: RMSE 0.892
ExtraTreesRegressor: RMSE 0.947
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Super Learner: RMSE 0.810
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\end{sphinxuseclass}
\sphinxAtStartPar
In the second step we use the super learner algorithm to estimate the expected value of the outcome using the treatment and confounders as predictors. Within this, there are three steps:
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
predict with the interventions

\item {} 
\sphinxAtStartPar
predict with every sample receiving no treatment

\item {} 
\sphinxAtStartPar
predict with every sample receiving treatment.

\end{enumerate}

\sphinxAtStartPar
We can take the difference of the last two as an ATE estimate, which is effectively the g\sphinxhyphen{}estimation approach. We see below this provides a decent 1st estimate.

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{df\PYGZus{}predict} \PYG{o}{=} \PYG{n}{df\PYGZus{}categ}\PYG{o}{.}\PYG{n}{copy}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{X} \PYG{o}{=} \PYG{n}{df\PYGZus{}predict}\PYG{p}{[}\PYG{n}{df\PYGZus{}predict}\PYG{o}{.}\PYG{n}{columns}\PYG{o}{.}\PYG{n}{drop}\PYG{p}{(}\PYG{p}{[}\PYG{n}{Y}\PYG{p}{]}\PYG{p}{)}\PYG{p}{]}\PYG{o}{.}\PYG{n}{to\PYGZus{}numpy}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{Q\PYGZus{}a} \PYG{o}{=} \PYG{n}{super\PYGZus{}learner\PYGZus{}predictions}\PYG{p}{(}\PYG{n}{X}\PYG{p}{,} \PYG{n}{models}\PYG{p}{,} \PYG{n}{meta\PYGZus{}model}\PYG{p}{)}
\PYG{n}{df\PYGZus{}predict}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{intervention}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{o}{=} \PYG{l+m+mi}{0}
\PYG{n}{X} \PYG{o}{=} \PYG{n}{df\PYGZus{}predict}\PYG{p}{[}\PYG{n}{df\PYGZus{}predict}\PYG{o}{.}\PYG{n}{columns}\PYG{o}{.}\PYG{n}{drop}\PYG{p}{(}\PYG{p}{[}\PYG{n}{Y}\PYG{p}{]}\PYG{p}{)}\PYG{p}{]}\PYG{o}{.}\PYG{n}{to\PYGZus{}numpy}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{Q\PYGZus{}0} \PYG{o}{=} \PYG{n}{super\PYGZus{}learner\PYGZus{}predictions}\PYG{p}{(}\PYG{n}{X}\PYG{p}{,} \PYG{n}{models}\PYG{p}{,} \PYG{n}{meta\PYGZus{}model}\PYG{p}{)}
\PYG{n}{df\PYGZus{}predict}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{intervention}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{o}{=} \PYG{l+m+mi}{1}
\PYG{n}{X} \PYG{o}{=} \PYG{n}{df\PYGZus{}predict}\PYG{p}{[}\PYG{n}{df\PYGZus{}predict}\PYG{o}{.}\PYG{n}{columns}\PYG{o}{.}\PYG{n}{drop}\PYG{p}{(}\PYG{p}{[}\PYG{n}{Y}\PYG{p}{]}\PYG{p}{)}\PYG{p}{]}\PYG{o}{.}\PYG{n}{to\PYGZus{}numpy}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{Q\PYGZus{}1} \PYG{o}{=} \PYG{n}{super\PYGZus{}learner\PYGZus{}predictions}\PYG{p}{(}\PYG{n}{X}\PYG{p}{,} \PYG{n}{models}\PYG{p}{,} \PYG{n}{meta\PYGZus{}model}\PYG{p}{)}

\PYG{n}{df\PYGZus{}tmle} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{DataFrame}\PYG{p}{(}\PYG{p}{[}\PYG{n}{df\PYGZus{}categ}\PYG{p}{[}\PYG{n}{Y}\PYG{p}{]}\PYG{o}{.}\PYG{n}{to\PYGZus{}numpy}\PYG{p}{(}\PYG{p}{)}\PYG{p}{,}\PYG{n}{df\PYGZus{}categ}\PYG{p}{[}\PYG{n}{T}\PYG{p}{]}\PYG{o}{.}\PYG{n}{to\PYGZus{}numpy}\PYG{p}{(}\PYG{p}{)}\PYG{p}{,} \PYG{n}{Q\PYGZus{}a}\PYG{p}{,}\PYG{n}{Q\PYGZus{}0}\PYG{p}{,}\PYG{n}{Q\PYGZus{}1}\PYG{p}{]}\PYG{p}{)}\PYG{o}{.}\PYG{n}{T}
\PYG{n}{df\PYGZus{}tmle}\PYG{o}{.}\PYG{n}{columns} \PYG{o}{=} \PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Y}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{D}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Q\PYGZus{}a}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Q\PYGZus{}0}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Q\PYGZus{}1}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}

\PYG{n}{df\PYGZus{}tmle}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Q\PYGZus{}1}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{p}{)} \PYG{o}{\PYGZhy{}} \PYG{n}{df\PYGZus{}tmle}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Q\PYGZus{}0}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
0.4192536456208886
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxAtStartPar
In the third step we obtain propensity scores (ps) and form a “clever covariate” from these values which will be used to refine our model. the inverse ps with indicator is added with the negative inverse of not being treated (1\sphinxhyphen{}ps) also multiplied with indicator if not being treated:
\begin{equation*}
\begin{split}H(D,X) = \frac{I(D=1)}{\hat \pi (X_i)} - \frac{I(D=0)}{1 - \hat \pi (X_i)}\end{split}
\end{equation*}
\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{T} \PYG{o}{=} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{intervention}\PYG{l+s+s1}{\PYGZsq{}}
\PYG{n}{Y} \PYG{o}{=} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{achievement\PYGZus{}score}\PYG{l+s+s1}{\PYGZsq{}}
\PYG{n}{X} \PYG{o}{=} \PYG{n}{df\PYGZus{}categ}\PYG{o}{.}\PYG{n}{columns}\PYG{o}{.}\PYG{n}{drop}\PYG{p}{(}\PYG{p}{[}\PYG{n}{T}\PYG{p}{,} \PYG{n}{Y}\PYG{p}{]}\PYG{p}{)}
\PYG{n}{ate}\PYG{p}{,} \PYG{n}{ps}\PYG{p}{,}\PYG{n}{\PYGZus{}} \PYG{o}{=} \PYG{n}{IPW}\PYG{p}{(}\PYG{n}{df\PYGZus{}categ}\PYG{p}{,} \PYG{n}{X}\PYG{p}{,} \PYG{n}{T}\PYG{p}{,} \PYG{n}{Y}\PYG{p}{)}

\PYG{n}{df\PYGZus{}tmle}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{H\PYGZus{}1}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{o}{=} \PYG{l+m+mi}{1}\PYG{o}{/}\PYG{n}{ps}
\PYG{n}{df\PYGZus{}tmle}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{H\PYGZus{}0}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{o}{=} \PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{o}{/}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{o}{\PYGZhy{}}\PYG{n}{ps}\PYG{p}{)}
\PYG{n}{df\PYGZus{}tmle}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{H\PYGZus{}a}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{o}{=} \PYG{n}{df\PYGZus{}tmle}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{D}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{o}{*} \PYG{n}{df\PYGZus{}tmle}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{H\PYGZus{}1}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{o}{+} \PYG{p}{(}\PYG{l+m+mi}{1}\PYG{o}{\PYGZhy{}}\PYG{n}{df\PYGZus{}tmle}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{D}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{)} \PYG{o}{*} \PYG{n}{df\PYGZus{}tmle}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{H\PYGZus{}0}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\sphinxAtStartPar
In the fourth and fifth steps, we estimate the fluctuation parameter using the logit function:
\begin{equation*}
\begin{split}logit(\mathbb{E}[Y|D,X]) = logit(\hat{\mathbb{E}}[Y|D,X]) = \epsilon H(D,X)\end{split}
\end{equation*}
\sphinxAtStartPar
We then update out initial estimates with the fluctuation parameter adjustment.

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{eps\PYGZus{}fit} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{polyfit}\PYG{p}{(}\PYG{n}{df\PYGZus{}tmle}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{H\PYGZus{}a}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{,} \PYG{n}{df\PYGZus{}tmle}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Y}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{o}{\PYGZhy{}} \PYG{n}{df\PYGZus{}tmle}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Q\PYGZus{}a}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}
\PYG{n}{df\PYGZus{}tmle}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Q\PYGZus{}0\PYGZus{}hat}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{o}{=} \PYG{n}{df\PYGZus{}tmle}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Q\PYGZus{}0}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{o}{+} \PYG{n}{eps\PYGZus{}fit} \PYG{o}{*} \PYG{n}{df\PYGZus{}tmle}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{H\PYGZus{}0}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}
\PYG{n}{df\PYGZus{}tmle}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Q\PYGZus{}1\PYGZus{}hat}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{o}{=} \PYG{n}{df\PYGZus{}tmle}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Q\PYGZus{}1}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{o}{+} \PYG{n}{eps\PYGZus{}fit} \PYG{o}{*} \PYG{n}{df\PYGZus{}tmle}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{H\PYGZus{}1}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}
\PYG{n}{df\PYGZus{}tmle}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Q\PYGZus{}a\PYGZus{}hat}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{o}{=} \PYG{n}{df\PYGZus{}tmle}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Q\PYGZus{}a}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{o}{+} \PYG{n}{eps\PYGZus{}fit} \PYG{o}{*} \PYG{n}{df\PYGZus{}tmle}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{H\PYGZus{}a}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{TMLE\PYGZus{}ate} \PYG{o}{=} \PYG{n}{df\PYGZus{}tmle}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Q\PYGZus{}1\PYGZus{}hat}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{p}{)} \PYG{o}{\PYGZhy{}} \PYG{n}{df\PYGZus{}tmle}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Q\PYGZus{}0\PYGZus{}hat}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{TMLE TAE estimate: }\PYG{l+s+si}{\PYGZob{}:.4f\PYGZcb{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{o}{.}\PYG{n}{format}\PYG{p}{(}\PYG{n}{TMLE\PYGZus{}ate}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
TMLE TAE estimate: 0.3807
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxAtStartPar
We see how the fluctuation adjusted outcomes estimates vastly improves the ATE to be more in line with AIPW. One major benefit of TMLE is a whole set of nice statistical and convergence properties. In this case, we can use something called the influence function to calculate a closed form standard error, unlike the empirical error estimates we gained by bootstrapping in the AIPW case.
\begin{equation*}
\begin{split}\hat{IF} = (Y-\hat{\mathbb{E}}*[Y|D,X])H(D,X) + \hat{\mathbb{E}}*[Y|D=1,X] - \hat{\mathbb{E}}*[Y|D=0,X] - ATE_{TMLE}\end{split}
\end{equation*}\begin{equation*}
\begin{split}SE = \sqrt{var(IF)/N}\end{split}
\end{equation*}
\sphinxAtStartPar
Using the above method, we see we get a SE very similar to our AIPW empirical methods.

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{IF} \PYG{o}{=} \PYG{p}{(}\PYG{n}{df\PYGZus{}tmle}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Y}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{o}{\PYGZhy{}} \PYG{n}{df\PYGZus{}tmle}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Q\PYGZus{}a\PYGZus{}hat}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{)} \PYG{o}{*} \PYG{n}{df\PYGZus{}tmle}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{H\PYGZus{}a}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{o}{+} \PYG{n}{df\PYGZus{}tmle}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Q\PYGZus{}1\PYGZus{}hat}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{o}{\PYGZhy{}} \PYG{n}{df\PYGZus{}tmle}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Q\PYGZus{}0\PYGZus{}hat}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{o}{\PYGZhy{}} \PYG{n}{TMLE\PYGZus{}ate}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{SE calculated from influcence function: }\PYG{l+s+si}{\PYGZob{}:.4f\PYGZcb{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{o}{.}\PYG{n}{format}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{sqrt}\PYG{p}{(}\PYG{n}{IF}\PYG{o}{.}\PYG{n}{var}\PYG{p}{(}\PYG{p}{)}\PYG{o}{/}\PYG{n}{df\PYGZus{}tmle}\PYG{o}{.}\PYG{n}{shape}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{)}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
SE calculated from influcence function: 0.0164
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}

\subsection{References}
\label{\detokenize{DR_Methods:references}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
1. Glynn, A. N., \PYGZam{} Quinn, K. M. (2010). An introduction to the augmented inverse propensity weighted estimator. Political analysis, 18(1), 36\PYGZhy{}56.

2. https://matheusfacure.github.io/python\PYGZhy{}causality\PYGZhy{}handbook/12\PYGZhy{}Doubly\PYGZhy{}Robust\PYGZhy{}Estimation.html

3. Gruber S, van der Laan MJ. Targeted minimum loss based estimator that outperforms a given estimator. Int J Biostat. 2012 May 18;8(1):Article 11. doi: 10.1515/1557\PYGZhy{}4679.1332. PMID: 22628356; PMCID: PMC6052865.
\end{sphinxVerbatim}

\sphinxstepscope


\chapter{FWL Theorem and Double Machine Learning}
\label{\detokenize{orthogonal_DML:fwl-theorem-and-double-machine-learning}}\label{\detokenize{orthogonal_DML::doc}}
\sphinxAtStartPar
Following up on the previous notebook where we covered several doubly robust methods (e.g., AIPW and TMLE), we will go through double machine learning (DML) in detail in this notebook. But before diving into theory let us understand why we need DML in the first place, shall we?

\sphinxAtStartPar
Augmented inverse propensity weighting (AIPW) is a modification of the inverse propensity weighting (IPW) that guarantees double roubustness and consistent average treatment effect(ATE) estimate even if 1) treatment/exposure model (\( \hat\pi(x) \)) or 2) outcome model (\( \hat\mu(x) \)) is misspecified \sphinxhref{https://www.law.berkeley.edu/files/AIPW(1).pdf}{GLynn and Quinn, 2009}. Although AIPW provides a nice flexibility in estimating a consistent ATE, it does necessitate at least one model to be correctly specified. If both the models are incorrectly specified, the naive IPW outperforms AIPW. Similarly, targeted maximum likelihood estimation (TMLE) is a semiparametric estimation framework. TMLE tends to work well when the treatment is not a weak predictor of the outcome. If that’s not the case, the estimation tends to be biased toward zero which obviously might not be the baseline truth.

\sphinxAtStartPar
The main objective of DML is to provide a general framework to estimating and performing inference on low\sphinxhyphen{}dimensional parameter (\( \theta_0\)) in presence of high\sphinxhyphen{}dimensional nuisance parameter utilizing nonparametric machine learning methods. DML works for both binary and continuous treatment variables which is not the case for some of the doubly robust methods. As the name suggests, DML leverages “double” or two high\sphinxhyphen{}performance ML methods to estimate a high\sphinxhyphen{}quality \(\theta_0\). Specifically, the first ML algorithm is used for treatment model while the second algorithm is used for the outcome model. Finally, Frisch\sphinxhyphen{}Waugh\sphinxhyphen{}Lovell (FWL)\sphinxhyphen{}type residuals\sphinxhyphen{}on\sphinxhyphen{}residuals regressioin is utilized to get a de\sphinxhyphen{}biased estimate of \(\theta_0\). DML is also know as “debiased\sphinxhyphen{}ML” or “orthogonalized ML.”


\section{FWL Theorem/Orthogonalization}
\label{\detokenize{orthogonal_DML:fwl-theorem-orthogonalization}}
\sphinxAtStartPar
Orthogonalization (or equivalently FWL Theorem (\sphinxhref{https://www.jstor.org/stable/pdf/1907330.pdf}{Frisch and Waugh, 1933}; \sphinxhref{https://www.tandfonline.com/doi/abs/10.3200/jece.39.1.88-91}{Lovell, 1963}) is the backbone of DML. Its principled approach guarantees an unbiased estimate. Since it is a key to understanding why DML works, we will first prove the FWL Theorem and implement it in an example to demonstrate how it debiases the data before moving on to the details of DML.

\sphinxAtStartPar
Let us take a multivariate linear regression
\begin{equation*}
\begin{split} Y = D_1\beta_1 + X\beta_2 + \epsilon \end{split}
\end{equation*}
\sphinxAtStartPar
where \(Y\) is \( n \times 1\) outcome, \(D\) is \(n \times p_1\) treatment variables, and \(X\) is \(n \times p_2\) covariates or nuisance parameters.

\sphinxAtStartPar
Multiply the equation with residual maker function (\(G\)) of the treatment parameters \(D\). The residual maker is defined by \(Gy = y - D(D'D)^{-1}D'y \equiv y - D\beta \equiv \epsilon_D\)
\begin{equation*}
\begin{split} GY = GD_1\beta_1 + GX\beta_2 + G\epsilon \end{split}
\end{equation*}
\sphinxAtStartPar
Since \(GD_1\beta_1 \equiv 0\), the equation above simplifies to
\begin{equation*}
\begin{split} GY = GX\beta_2 + G\epsilon \end{split}
\end{equation*}
\sphinxAtStartPar
Now, the final equation becomes
\begin{equation*}
\begin{split} GY = GX\beta_2 + \epsilon \end{split}
\end{equation*}
\sphinxAtStartPar
Taking a closer look at the equation, we can see that we are regressing residuals on residuals. This shows that results obtained from multivariate linear regression is the same as the residuals\sphinxhyphen{}on\sphinxhyphen{}residuals regression.

\sphinxAtStartPar
Now that we have seen a proof of why residuals\sphinxhyphen{}on\sphinxhyphen{}residuals regression work, let us go through an example implementation to see orthogonalization in action.

\begin{sphinxuseclass}{cell}
\begin{sphinxuseclass}{tag_hide-input}\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
/Users/sunaybhat/miniconda3/envs/stats256/lib/python3.10/site\PYGZhy{}packages/xgboost/compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.
  from pandas import MultiIndex, Int64Index
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\end{sphinxuseclass}
\begin{sphinxuseclass}{cell}
\begin{sphinxuseclass}{tag_hide-input}
\end{sphinxuseclass}
\end{sphinxuseclass}

\subsection{Orthogonalization: Example}
\label{\detokenize{orthogonal_DML:orthogonalization-example}}
\sphinxAtStartPar
To demonstrate how orthogonalization debiases the data, we will use a simulated data on ice cream sales. The outcome (\(Y\)) is the number of sales, the treatment is price, and the covariates (\(X\)) are temperature, weekday (categorical variable) and cost of the ice cream.

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{df\PYGZus{}ortho} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{read\PYGZus{}csv}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{ice\PYGZus{}cream\PYGZus{}sales.csv}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{df\PYGZus{}ortho}\PYG{o}{.}\PYG{n}{head}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
   temp  weekday  cost  price  sales
0  17.3        6   1.5    5.6    173
1  25.4        3   0.3    4.9    196
2  23.3        5   1.5    7.6    207
3  26.9        1   0.3    5.3    241
4  20.2        1   1.0    7.2    227
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxAtStartPar
There are no missing data as we can see below:

\begin{sphinxuseclass}{cell}
\begin{sphinxuseclass}{tag_hide-input}\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
temp       0
weekday    0
cost       0
price      0
sales      0
dtype: int64
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\end{sphinxuseclass}
\sphinxAtStartPar
The figure below shows a heatmap of the Pearsons’ correlation plot of the dataset. The correlation plot shows positive linear relationship between three pairs of variables (sales\sphinxhyphen{}temp, cost\sphinxhyphen{}price, and price\sphinxhyphen{}sales)\sphinxhyphen{} two of which makes sense, one does not. As the temperature increases, we often expect ice cream sales to increase because people buy more ice cream if it is hot. Similarly, the price of the ice crease will increase if the purchase cost for the vendor is high. However, the third positive correlation is between price and sales which necessarily doesn’t make sense because if the price is high, people tend to buy less so if anything, there should be a negative correlation. The positive correlation could potentially because of bias.

\begin{sphinxuseclass}{cell}
\begin{sphinxuseclass}{tag_hide-input}\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{orthogonal_DML_9_0}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\end{sphinxuseclass}
\sphinxAtStartPar
Looking at the scatter plot between sales and price, we can see that the data clearly is biased. First, we can see the two distinct cluster. On weekends, the sales is high because more people go outside which increases the demand. The vendors likely take an advantage of the increased demands and hike up the prices which ultimately reduces the sales. However, the sales appear to be roughly uniform regardless of the price during weekdays. The higher sales on weekends and the consistent sales during weekdays gives a positive relationship between sales and price as shows by a linear fit line (red line) in the figure below.

\begin{sphinxuseclass}{cell}
\begin{sphinxuseclass}{tag_hide-input}\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{orthogonal_DML_11_0}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\end{sphinxuseclass}
\sphinxAtStartPar
To debiase the data, we need two models\sphinxhyphen{} treatment and outcome model. The treatment model debiases the bias induced in price using all the other confounders, while the outcome model debiases the bias in sales introduced by the same covariates. Consistent with FWL Theorem, we used OLS to create the treatment and outcome models as shown below:

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{}create a treatment model }
\PYG{n}{model\PYGZus{}treatment} \PYG{o}{=} \PYG{n}{smf}\PYG{o}{.}\PYG{n}{ols}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{price \PYGZti{} cost + C(weekday) + temp}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{data}\PYG{o}{=}\PYG{n}{df\PYGZus{}ortho}\PYG{p}{)}\PYG{o}{.}\PYG{n}{fit}\PYG{p}{(}\PYG{p}{)}
\PYG{c+c1}{\PYGZsh{}create an outcome model}
\PYG{n}{model\PYGZus{}outcome} \PYG{o}{=} \PYG{n}{smf}\PYG{o}{.}\PYG{n}{ols}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{sales \PYGZti{} cost + C(weekday) + temp}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{data}\PYG{o}{=}\PYG{n}{df\PYGZus{}ortho}\PYG{p}{)}\PYG{o}{.}\PYG{n}{fit}\PYG{p}{(}\PYG{p}{)}

\PYG{n}{debiased\PYGZus{}df\PYGZus{}ortho} \PYG{o}{=} \PYG{n}{df\PYGZus{}ortho}\PYG{o}{.}\PYG{n}{assign}\PYG{p}{(}\PYG{o}{*}\PYG{o}{*}\PYG{p}{\PYGZob{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{resid\PYGZus{}output\PYGZus{}sales}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:}\PYG{n}{model\PYGZus{}outcome}\PYG{o}{.}\PYG{n}{resid}\PYG{p}{,}
                                       \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{resid\PYGZus{}treatment\PYGZus{}price}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:}\PYG{n}{model\PYGZus{}treatment}\PYG{o}{.}\PYG{n}{resid}\PYG{p}{\PYGZcb{}}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\sphinxAtStartPar
If we plot price\sphinxhyphen{}residuals against sales, we can see that we have debiased the bias in price. First, we have uncovered the negative relationship between the price and sales as expected. Most importantly, we can see that decline in sales during the weekend is consistent and not necessarily depending on the price. In the raw data above, we saw that as the price increased, the sales decreased drastically, thus inducing price bias. But in this case, the number of sales on the left and the right side of price\sphinxhyphen{}residual roughly appears to be the same.

\sphinxAtStartPar
Even though we have debiased the price, we can still see that the data has two distinct clusters as a function of the sale day. During weekends, the sales seems to be higher as compared to the weekdays.

\begin{sphinxuseclass}{cell}
\begin{sphinxuseclass}{tag_hide-input}\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{orthogonal_DML_15_0}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\end{sphinxuseclass}
\sphinxAtStartPar
Finally, lets see if we can debiase the bias in sales amount. The figure below plots sales\sphinxhyphen{}residuals against price\sphinxhyphen{}residuals. We can see that the we no longer have distinct clusters of data neither do we see a dramatic decline in sales as the price increase. The slope of the linear fit line (red line) is the debiased estimated of ATE that is obtained by regressing price\sphinxhyphen{}residuals on sales\sphinxhyphen{}residuals.

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{figure}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{sns}\PYG{o}{.}\PYG{n}{scatterplot}\PYG{p}{(}\PYG{n}{data}\PYG{o}{=}\PYG{n}{debiased\PYGZus{}df\PYGZus{}ortho}\PYG{p}{,} \PYG{n}{x}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{resid\PYGZus{}treatment\PYGZus{}price}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{y}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{resid\PYGZus{}output\PYGZus{}sales}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
                \PYG{n}{hue}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{weekday}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{palette}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{RdYlGn}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{legend}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{full}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{sns}\PYG{o}{.}\PYG{n}{regplot}\PYG{p}{(}\PYG{n}{x}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{resid\PYGZus{}treatment\PYGZus{}price}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{y}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{resid\PYGZus{}output\PYGZus{}sales}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{data}\PYG{o}{=}\PYG{n}{debiased\PYGZus{}df\PYGZus{}ortho}\PYG{p}{,}
            \PYG{n}{scatter}\PYG{o}{=}\PYG{k+kc}{False}\PYG{p}{,} \PYG{n}{ci}\PYG{o}{=}\PYG{k+kc}{False}\PYG{p}{,} \PYG{n}{color} \PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{red}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{xlabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Price Residuals}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{16}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{ylabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Sales Residuals}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{16}\PYG{p}{)}
\PYG{c+c1}{\PYGZsh{} plt.savefig(\PYGZsq{}scatter\PYGZus{}doubleDebiased\PYGZus{}iceCream.png\PYGZsq{}, bbox\PYGZus{}inches=\PYGZsq{}tight\PYGZsq{})}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{show}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{orthogonal_DML_17_0}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxAtStartPar
This example illustrates the role orthogonalization playing in debiasing the data and more importantly estimating a debiased ATE estimate.


\section{Debiased/Double Machine Learning}
\label{\detokenize{orthogonal_DML:debiased-double-machine-learning}}
\sphinxAtStartPar
Now that we understand orthogonalization, we will dig a little deeper into DML formulations. Good news is that if you understand the intuition behind orthogonalization, you already understand DML. To simplify, DML is fundamentally the same as orthogonalization except that advanced ML algorithms are used to model treatment and outcome instead of OLS.

\sphinxAtStartPar
One may wonder, ML algorithms are widely used in anything and everything, haven’t researchers used to estimate treatment effect? Yes, they have and that’s when naive approach comes in. In this section, naive approach refers to methods involving ML methods with no modifications. Since DML is a slighly different class of roubly\sphinxhyphen{}robust method in a sense that it utilized ML algorith, we introduce Naive or prediction\sphinxhyphen{}based ML approach to make a direct comparison against DML and show why DML is better than the naive approach.

\sphinxAtStartPar
To be consistent with the \sphinxhref{https://arxiv.org/abs/1608.00060}{Double/Debiased Machine Learning Paper}(Chernozhukov et al., 2018), we will use the same notations.

\sphinxAtStartPar
Let us take a partial linear regression,
\begin{equation*}
\begin{split}Y = D\theta_0 + g_0(X) + U, \quad E[U|X, D] = 0 \end{split}
\end{equation*}\begin{equation*}
\begin{split}D = m_0(X) + V, \quad E[V|X] = 0\end{split}
\end{equation*}
\sphinxAtStartPar
where \(Y\) is the outcome, \(D\) is the treatment, \(X\) is the covariates/confounders, and \(U\) and \(V\) are the noise. The quantity of interest is the regression coefficient, \(\theta_0\).

\sphinxAtStartPar
Under naive approach, the following steps are undertaken to estimate the \(\theta_0\)
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
Predict \(Y\) using \(D\) and \(X\). This gives you \(\hat Y\) in the form of \(D\hat\theta_0 + \hat g_0(X)\)

\item {} 
\sphinxAtStartPar
Run advanced ML algorithm (e.g., Random Forest) of \(Y - D\hat\theta_0\) on \(X\) to fit \(\hat g_0(X)\)

\item {} 
\sphinxAtStartPar
Run OLS of \(Y - \hat g_0(X)\) on \(D\) to fit \(\hat\theta_0\). In other words, \(\hat\theta_0\) is given by:

\end{enumerate}
\begin{equation*}
\begin{split} \hat\theta_0 = \left( \frac{1}{n} \sum_{i \in I} D_i^2 \right)^{-1} \frac{1}{n} \sum_{i \in I}D_i(Y_i - \hat g(X_i)) \end{split}
\end{equation*}
\sphinxAtStartPar
The naive approach displays excellent predictive performance but introduces a regularization bias in learning \(g_0\). Lets take a closer look at the decomposition of the estimation error in \(\hat\theta_0\) to islolate the regularization bias,
\begin{equation*}
\begin{split}\sqrt{n}(\hat\theta_0 - \theta_0) = \underbrace{\left( \frac{1}{n} \sum_{i \in I} D_i^2 \right)^{-1} \frac{1}{\sqrt{n}} \sum_{i \in I} D_i U_i}_{:a} + \underbrace{\left( \frac{1}{n} \sum_{i \in I} D_i^2 \right)^{-1} \frac{1}{\sqrt{n}} \sum_{i \in I} D_i(g_0(X_i) - \hat g_0(X_i))}_{:b}\end{split}
\end{equation*}
\sphinxAtStartPar
The first term \(a\) is well\sphinxhyphen{}behaved under mild conditions and has zero mean \(a \rightsquigarrow N(0, \bar\Sigma)\) for some \(\bar\Sigma\). However, the regularization term (\(b\)) does not center around 0, and in fact diverges for the majority of the ML algorithms. The regularization bias is addressed using orthogonalization. How exactly does DML do it? Using the following three steps:
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
Predict \(Y\) and \(D\) using \(X\) using the advanced ML methods to obtain \(\widehat{E[Y|X]}\) and \(\widehat{E[D|X]}\)

\item {} 
\sphinxAtStartPar
Obtain residuals from the two models i.e. \(\widehat{W} = Y -\widehat{E[Y|X]}\) and \(\widehat{V} = D -\widehat{E[D|X]}\)

\item {} 
\sphinxAtStartPar
Use orthogonalization, i.e. regress \(\widehat{W}\) on \(\widehat{V}\) to get \(\hat\theta_0\)

\end{enumerate}

\sphinxAtStartPar
In DML, the estimation error in \(\hat\theta_0\) can be decomposed into
\begin{equation*}
\begin{split}\sqrt{n}(\hat\theta_0-\theta_0) = a^* + b^* + c^*\end{split}
\end{equation*}
\sphinxAtStartPar
where,
\$\(a^* = (EV^2)^{-1}\frac{1}{\sqrt{n}}\sum_{i \in I}V_iU_i \rightsquigarrow N(0, \Sigma),\)\$
\begin{equation*}
\begin{split}b^* = (EV^2)^{-1}\frac{1}{\sqrt{n}}\sum_{i \in I}(\hat m_0(X_i) - m_0(X_i))(\hat g_0(X_i) - g_0(X_i)) \end{split}
\end{equation*}
\sphinxAtStartPar
and,
\begin{equation*}
\begin{split} c^*  = o_P(1)\end{split}
\end{equation*}
\sphinxAtStartPar
Similar to naive approach, \(a^*\) has a zero mean. The second term, \(b^*\) also nearly as zero mean because the the high predictive performance of advanced ML algorithms ensure that the product of the estimation error in \(\hat m_0\) and \(\hat g_0\) nearly vanishes to zero. The \(c^*\) term represents bias induced due to overfitting, which is sufficiently well\sphinxhyphen{}behaved and vanishes in probability under sample splitting. For DML to be doubly robust, it is paramount to split the data into multiple folds while estimating \(\hat\theta_0\). For a detailed proof on why \(c^*\) vanishes in probability in presence of sample splitting, we invite readers to read the \sphinxhref{https://arxiv.org/abs/1608.00060}{Double/Debiased Machine Learning Paper}.


\subsection{DML: Example Implementation}
\label{\detokenize{orthogonal_DML:dml-example-implementation}}
\sphinxAtStartPar
In this example, we will use the data on 401(k) eligibility (treatment variable) on the total accumulated net financial assess (net\_tfa). This dataset was assembled from the 1991 Survey of Income and Porgram Participation. Since the assignment was not random, the DML is implemented to negate bias due to non\sphinxhyphen{}randomness assignment.

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{df\PYGZus{}401k} \PYG{o}{=} \PYG{n}{fetch\PYGZus{}401K}\PYG{p}{(}\PYG{n}{return\PYGZus{}type}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{DataFrame}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{df\PYGZus{}401k}\PYG{o}{.}\PYG{n}{sample}\PYG{p}{(}\PYG{n}{n}\PYG{o}{=}\PYG{l+m+mi}{5}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
         nifa  net\PYGZus{}tfa       tw  age      inc  fsize  educ  db  marr  twoearn  \PYGZbs{}
3322  13198.0  20418.0  50418.0   51  17616.0      3    12   1     1        0   
1526      0.0      0.0      0.0   43  17844.0      3     8   1     0        0   
5507      0.0      0.0   1500.0   43  19074.0      1    14   0     0        0   
15        0.0  \PYGZhy{}1000.0   \PYGZhy{}325.0   29  14580.0      3    12   0     0        0   
3687   6000.0  12800.0  12800.0   31  50529.0      2    16   0     1        1   

      e401  p401  pira  hown  
3322     0     0     1     1  
1526     0     0     0     0  
5507     0     0     0     0  
15       0     0     0     0  
3687     0     0     1     1  
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxAtStartPar
The description of the features are highlighted below:
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{age:} age of the employee

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{inc:} income amount

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{fsize:} family size

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{educ:} years of education

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{marr:} marriage indicator (1: married, 0: otherwise)

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{twoearn:} two\sphinxhyphen{}earner status indicator in the family

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{db:} a defined benefit pension status indicator

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{pira:} an IRA participation indicator

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{hown:} a home ownership indicator

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{net\_tfa:} net total financial assets. Defined as the sum fo IRA balances, 401(k) balances, checking accounts, U.S. saving bonds, stocks, mutual funds, etc.

\end{enumerate}

\sphinxAtStartPar
As discussed below, we will not use all of the features in this example.

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{df\PYGZus{}401k}\PYG{o}{.}\PYG{n}{shape}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
(9915, 14)
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxAtStartPar
Difference in mean between the employees who were eligible vs not eligible.

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{df\PYGZus{}401k}\PYG{o}{.}\PYG{n}{groupby}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{e401}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{net\PYGZus{}tfa}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
e401
0    10788.044922
1    30347.388672
Name: net\PYGZus{}tfa, dtype: float32
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxAtStartPar
Difference in mean between the employees who opted to participate in 401(k) vs those that did not participate.

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{df\PYGZus{}401k}\PYG{o}{.}\PYG{n}{groupby}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{p401}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{net\PYGZus{}tfa}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
p401
0    10890.477539
1    38262.058594
Name: net\PYGZus{}tfa, dtype: float32
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxAtStartPar
For consistency, we will use the same covariates used in the \sphinxhref{https://arxiv.org/abs/1608.00060}{Double/Debiased Machine Learning Paper}.

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{features\PYGZus{}cherno} \PYG{o}{=} \PYG{p}{[} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{age}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{inc}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{fsize}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{educ}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{marr}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{twoearn}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{db}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{pira}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{hown}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{}outcome model }
\PYG{n}{my} \PYG{o}{=} \PYG{n}{smf}\PYG{o}{.}\PYG{n}{ols}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{net\PYGZus{}tfa \PYGZti{} }\PYG{l+s+s1}{\PYGZsq{}} \PYG{o}{+} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{+}\PYG{l+s+s1}{\PYGZsq{}}\PYG{o}{.}\PYG{n}{join}\PYG{p}{(}\PYG{n}{features\PYGZus{}cherno}\PYG{p}{)}\PYG{p}{,} \PYG{n}{data} \PYG{o}{=} \PYG{n}{df\PYGZus{}401k}\PYG{p}{)}\PYG{o}{.}\PYG{n}{fit}\PYG{p}{(}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{}treatment model}
\PYG{n}{mt} \PYG{o}{=} \PYG{n}{smf}\PYG{o}{.}\PYG{n}{ols}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{e401 \PYGZti{}  }\PYG{l+s+s1}{\PYGZsq{}} \PYG{o}{+} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{+}\PYG{l+s+s1}{\PYGZsq{}}\PYG{o}{.}\PYG{n}{join}\PYG{p}{(}\PYG{n}{features\PYGZus{}cherno}\PYG{p}{)}\PYG{p}{,} \PYG{n}{data} \PYG{o}{=} \PYG{n}{df\PYGZus{}401k}\PYG{p}{)}\PYG{o}{.}\PYG{n}{fit}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\sphinxAtStartPar
One of the limitations we have not mentioned before is that the orthogonalization is limited to linear relationship between the covariates, treatment, and outcome models. Of course, the linear regression can be extended to a polynomial regression to capture nonlinear relationship but having to specify the nonlinear functional form is not that flexible and desirable. We implement linear and polynomial regression to shed light on the high\sphinxhyphen{}level predictive performance of commonly used ML algorithms.

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{orthogonal} \PYG{o}{=} \PYG{n}{smf}\PYG{o}{.}\PYG{n}{ols}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{tfa\PYGZus{}res\PYGZti{}e401\PYGZus{}res}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} 
        \PYG{n}{data}\PYG{o}{=}\PYG{n}{df\PYGZus{}401k}\PYG{o}{.}\PYG{n}{assign}\PYG{p}{(}\PYG{n}{tfa\PYGZus{}res}\PYG{o}{=}\PYG{n}{my}\PYG{o}{.}\PYG{n}{resid}\PYG{p}{,} \PYG{c+c1}{\PYGZsh{} sales residuals}
                          \PYG{n}{e401\PYGZus{}res}\PYG{o}{=}\PYG{n}{mt}\PYG{o}{.}\PYG{n}{resid}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} price residuals}
       \PYG{p}{)}\PYG{o}{.}\PYG{n}{fit}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{orthogonal}\PYG{o}{.}\PYG{n}{summary}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n}{tables}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZlt{}class \PYGZsq{}statsmodels.iolib.table.SimpleTable\PYGZsq{}\PYGZgt{}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\begin{sphinxuseclass}{cell}
\begin{sphinxuseclass}{tag_hide-input}\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
          coef    std err      2.5 \PYGZpc{}     97.5 \PYGZpc{}
0  5896.198421   1249.446   3447.029   8345.367
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\end{sphinxuseclass}
\sphinxAtStartPar
Creating a polynomial regression

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{my\PYGZus{}poly} \PYG{o}{=} \PYG{n}{smf}\PYG{o}{.}\PYG{n}{ols}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{net\PYGZus{}tfa \PYGZti{} age + inc + educ + fsize + marr + twoearn + db + pira + hown + age*fsize + educ*age + fsize**2}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}
                  \PYG{n}{data} \PYG{o}{=} \PYG{n}{df\PYGZus{}401k}\PYG{p}{)}\PYG{o}{.}\PYG{n}{fit}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{mt\PYGZus{}poly} \PYG{o}{=} \PYG{n}{smf}\PYG{o}{.}\PYG{n}{ols}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{e401 \PYGZti{} age + inc + educ + fsize + marr + twoearn + db + pira + hown}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{data} \PYG{o}{=} \PYG{n}{df\PYGZus{}401k}\PYG{p}{)}\PYG{o}{.}\PYG{n}{fit}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{polynomial} \PYG{o}{=} \PYG{n}{smf}\PYG{o}{.}\PYG{n}{ols}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{tfa\PYGZus{}res\PYGZti{}e401\PYGZus{}res}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} 
        \PYG{n}{data}\PYG{o}{=}\PYG{n}{df\PYGZus{}401k}\PYG{o}{.}\PYG{n}{assign}\PYG{p}{(}\PYG{n}{tfa\PYGZus{}res}\PYG{o}{=}\PYG{n}{my\PYGZus{}poly}\PYG{o}{.}\PYG{n}{resid}\PYG{p}{,} \PYG{c+c1}{\PYGZsh{} sales residuals}
                          \PYG{n}{e401\PYGZus{}res}\PYG{o}{=}\PYG{n}{mt\PYGZus{}poly}\PYG{o}{.}\PYG{n}{resid}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} price residuals}
       \PYG{p}{)}\PYG{o}{.}\PYG{n}{fit}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\sphinxAtStartPar
As we can see, for the polynomial regression, the estimated ATE is slighly higher by about \$200 as compared to OLS. This indicates that the data is highly nonlinear and we can leverage the ML algorithms to capture the nonlinear relationship.

\begin{sphinxuseclass}{cell}
\begin{sphinxuseclass}{tag_hide-input}\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
          coef    std err      2.5 \PYGZpc{}     97.5 \PYGZpc{}
0  6084.770503   1247.310   3639.789   8529.752
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\end{sphinxuseclass}
\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Initialize DoubleMLData (data\PYGZhy{}backend of DoubleML)}
\PYG{n}{data\PYGZus{}dml\PYGZus{}base} \PYG{o}{=} \PYG{n}{dml}\PYG{o}{.}\PYG{n}{DoubleMLData}\PYG{p}{(}\PYG{n}{df\PYGZus{}401k}\PYG{p}{,}
                                 \PYG{n}{y\PYGZus{}col}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{net\PYGZus{}tfa}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}
                                 \PYG{n}{d\PYGZus{}cols}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{e401}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}
                                 \PYG{n}{x\PYGZus{}cols}\PYG{o}{=}\PYG{n}{features\PYGZus{}cherno}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\sphinxAtStartPar
In the following section, Random Forest, Xtreme Gradient Boosting (XGBoost), and Regression trees are implemented to estimate the ATE in terms of the total financial asset. Two variations of implementation are provided (3 folds and 5 folds data splits) to highlight the role sample spliting plays in reducing the bias.

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Random Forest with 3 folds split}
\PYG{n}{randomForest} \PYG{o}{=} \PYG{n}{RandomForestRegressor}\PYG{p}{(}
    \PYG{n}{n\PYGZus{}estimators}\PYG{o}{=}\PYG{l+m+mi}{500}\PYG{p}{,} \PYG{n}{max\PYGZus{}depth}\PYG{o}{=}\PYG{l+m+mi}{7}\PYG{p}{,} \PYG{n}{max\PYGZus{}features}\PYG{o}{=}\PYG{l+m+mi}{3}\PYG{p}{,} \PYG{n}{min\PYGZus{}samples\PYGZus{}leaf}\PYG{o}{=}\PYG{l+m+mi}{3}\PYG{p}{)}
\PYG{n}{randomForest\PYGZus{}class} \PYG{o}{=} \PYG{n}{RandomForestClassifier}\PYG{p}{(}
    \PYG{n}{n\PYGZus{}estimators}\PYG{o}{=}\PYG{l+m+mi}{500}\PYG{p}{,} \PYG{n}{max\PYGZus{}depth}\PYG{o}{=}\PYG{l+m+mi}{5}\PYG{p}{,} \PYG{n}{max\PYGZus{}features}\PYG{o}{=}\PYG{l+m+mi}{4}\PYG{p}{,} \PYG{n}{min\PYGZus{}samples\PYGZus{}leaf}\PYG{o}{=}\PYG{l+m+mi}{7}\PYG{p}{)}

\PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{seed}\PYG{p}{(}\PYG{l+m+mi}{123}\PYG{p}{)}
\PYG{n}{dml\PYGZus{}plr\PYGZus{}forest} \PYG{o}{=} \PYG{n}{dml}\PYG{o}{.}\PYG{n}{DoubleMLPLR}\PYG{p}{(}\PYG{n}{data\PYGZus{}dml\PYGZus{}base}\PYG{p}{,}
                                 \PYG{n}{ml\PYGZus{}g} \PYG{o}{=} \PYG{n}{randomForest}\PYG{p}{,}
                                 \PYG{n}{ml\PYGZus{}m} \PYG{o}{=} \PYG{n}{randomForest\PYGZus{}class}\PYG{p}{,}
                                 \PYG{n}{n\PYGZus{}folds} \PYG{o}{=} \PYG{l+m+mi}{3}\PYG{p}{,} 
                                \PYG{n}{n\PYGZus{}rep}\PYG{o}{=}\PYG{l+m+mi}{10}\PYG{p}{)}
\PYG{n}{dml\PYGZus{}plr\PYGZus{}forest}\PYG{o}{.}\PYG{n}{fit}\PYG{p}{(}\PYG{n}{store\PYGZus{}predictions}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{)}
\PYG{n}{forest\PYGZus{}summary3} \PYG{o}{=} \PYG{n}{dml\PYGZus{}plr\PYGZus{}forest}\PYG{o}{.}\PYG{n}{summary}

\PYG{n}{forest\PYGZus{}summary3}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
             coef      std err         t         P\PYGZgt{}|t|       2.5 \PYGZpc{}  \PYGZbs{}
e401  9018.368261  1315.291812  6.856553  7.054183e\PYGZhy{}12  6440.44368   

            97.5 \PYGZpc{}  
e401  11596.292842  
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Random Forest with 5 folds split}
\PYG{n}{randomForest} \PYG{o}{=} \PYG{n}{RandomForestRegressor}\PYG{p}{(}
    \PYG{n}{n\PYGZus{}estimators}\PYG{o}{=}\PYG{l+m+mi}{500}\PYG{p}{,} \PYG{n}{max\PYGZus{}depth}\PYG{o}{=}\PYG{l+m+mi}{7}\PYG{p}{,} \PYG{n}{max\PYGZus{}features}\PYG{o}{=}\PYG{l+m+mi}{3}\PYG{p}{,} \PYG{n}{min\PYGZus{}samples\PYGZus{}leaf}\PYG{o}{=}\PYG{l+m+mi}{3}\PYG{p}{)}
\PYG{n}{randomForest\PYGZus{}class} \PYG{o}{=} \PYG{n}{RandomForestClassifier}\PYG{p}{(}
    \PYG{n}{n\PYGZus{}estimators}\PYG{o}{=}\PYG{l+m+mi}{500}\PYG{p}{,} \PYG{n}{max\PYGZus{}depth}\PYG{o}{=}\PYG{l+m+mi}{5}\PYG{p}{,} \PYG{n}{max\PYGZus{}features}\PYG{o}{=}\PYG{l+m+mi}{4}\PYG{p}{,} \PYG{n}{min\PYGZus{}samples\PYGZus{}leaf}\PYG{o}{=}\PYG{l+m+mi}{7}\PYG{p}{)}

\PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{seed}\PYG{p}{(}\PYG{l+m+mi}{123}\PYG{p}{)}
\PYG{n}{dml\PYGZus{}plr\PYGZus{}forest} \PYG{o}{=} \PYG{n}{dml}\PYG{o}{.}\PYG{n}{DoubleMLPLR}\PYG{p}{(}\PYG{n}{data\PYGZus{}dml\PYGZus{}base}\PYG{p}{,}
                                 \PYG{n}{ml\PYGZus{}g} \PYG{o}{=} \PYG{n}{randomForest}\PYG{p}{,}
                                 \PYG{n}{ml\PYGZus{}m} \PYG{o}{=} \PYG{n}{randomForest\PYGZus{}class}\PYG{p}{,}
                                 \PYG{n}{n\PYGZus{}folds} \PYG{o}{=} \PYG{l+m+mi}{5}\PYG{p}{,} 
                                \PYG{n}{n\PYGZus{}rep}\PYG{o}{=}\PYG{l+m+mi}{10}\PYG{p}{)}
\PYG{n}{dml\PYGZus{}plr\PYGZus{}forest}\PYG{o}{.}\PYG{n}{fit}\PYG{p}{(}\PYG{n}{store\PYGZus{}predictions}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{)}
\PYG{n}{forest\PYGZus{}summary5} \PYG{o}{=} \PYG{n}{dml\PYGZus{}plr\PYGZus{}forest}\PYG{o}{.}\PYG{n}{summary}

\PYG{n}{forest\PYGZus{}summary5}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
             coef      std err         t         P\PYGZgt{}|t|        2.5 \PYGZpc{}  \PYGZbs{}
e401  8961.175025  1309.593551  6.842715  7.770632e\PYGZhy{}12  6394.418831   

            97.5 \PYGZpc{}  
e401  11527.931219  
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Gradient Boosted Trees with 3 folds split}
\PYG{n}{boost} \PYG{o}{=} \PYG{n}{XGBRegressor}\PYG{p}{(}\PYG{n}{n\PYGZus{}jobs}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{objective} \PYG{o}{=} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{reg:squarederror}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
                     \PYG{n}{eta}\PYG{o}{=}\PYG{l+m+mf}{0.1}\PYG{p}{,} \PYG{n}{n\PYGZus{}estimators}\PYG{o}{=}\PYG{l+m+mi}{35}\PYG{p}{)}
\PYG{n}{boost\PYGZus{}class} \PYG{o}{=} \PYG{n}{XGBClassifier}\PYG{p}{(}\PYG{n}{use\PYGZus{}label\PYGZus{}encoder}\PYG{o}{=}\PYG{k+kc}{False}\PYG{p}{,} \PYG{n}{n\PYGZus{}jobs}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{,}
                            \PYG{n}{objective} \PYG{o}{=} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{binary:logistic}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{eval\PYGZus{}metric} \PYG{o}{=} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{logloss}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
                            \PYG{n}{eta}\PYG{o}{=}\PYG{l+m+mf}{0.1}\PYG{p}{,} \PYG{n}{n\PYGZus{}estimators}\PYG{o}{=}\PYG{l+m+mi}{34}\PYG{p}{)}

\PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{seed}\PYG{p}{(}\PYG{l+m+mi}{123}\PYG{p}{)}
\PYG{n}{dml\PYGZus{}plr\PYGZus{}boost} \PYG{o}{=} \PYG{n}{dml}\PYG{o}{.}\PYG{n}{DoubleMLPLR}\PYG{p}{(}\PYG{n}{data\PYGZus{}dml\PYGZus{}base}\PYG{p}{,}
                                \PYG{n}{ml\PYGZus{}g} \PYG{o}{=} \PYG{n}{boost}\PYG{p}{,}
                                \PYG{n}{ml\PYGZus{}m} \PYG{o}{=} \PYG{n}{boost\PYGZus{}class}\PYG{p}{,}
                                \PYG{n}{dml\PYGZus{}procedure}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{dml2}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}
                                \PYG{n}{n\PYGZus{}folds} \PYG{o}{=} \PYG{l+m+mi}{3}\PYG{p}{,}
                                \PYG{n}{n\PYGZus{}rep} \PYG{o}{=} \PYG{l+m+mi}{10}\PYG{p}{)}
\PYG{n}{dml\PYGZus{}plr\PYGZus{}boost}\PYG{o}{.}\PYG{n}{fit}\PYG{p}{(}\PYG{n}{store\PYGZus{}predictions}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{)}
\PYG{n}{boost\PYGZus{}summary3} \PYG{o}{=} \PYG{n}{dml\PYGZus{}plr\PYGZus{}boost}\PYG{o}{.}\PYG{n}{summary}

\PYG{n}{boost\PYGZus{}summary3}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
             coef      std err         t         P\PYGZgt{}|t|        2.5 \PYGZpc{}  \PYGZbs{}
e401  9002.744739  1399.883887  6.431065  1.267127e\PYGZhy{}10  6259.022737   

           97.5 \PYGZpc{}  
e401  11746.46674  
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Gradient Boosted Trees with 5 folds split}
\PYG{n}{boost} \PYG{o}{=} \PYG{n}{XGBRegressor}\PYG{p}{(}\PYG{n}{n\PYGZus{}jobs}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{objective} \PYG{o}{=} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{reg:squarederror}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
                     \PYG{n}{eta}\PYG{o}{=}\PYG{l+m+mf}{0.1}\PYG{p}{,} \PYG{n}{n\PYGZus{}estimators}\PYG{o}{=}\PYG{l+m+mi}{35}\PYG{p}{)}
\PYG{n}{boost\PYGZus{}class} \PYG{o}{=} \PYG{n}{XGBClassifier}\PYG{p}{(}\PYG{n}{use\PYGZus{}label\PYGZus{}encoder}\PYG{o}{=}\PYG{k+kc}{False}\PYG{p}{,} \PYG{n}{n\PYGZus{}jobs}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{,}
                            \PYG{n}{objective} \PYG{o}{=} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{binary:logistic}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{eval\PYGZus{}metric} \PYG{o}{=} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{logloss}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
                            \PYG{n}{eta}\PYG{o}{=}\PYG{l+m+mf}{0.1}\PYG{p}{,} \PYG{n}{n\PYGZus{}estimators}\PYG{o}{=}\PYG{l+m+mi}{34}\PYG{p}{)}

\PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{seed}\PYG{p}{(}\PYG{l+m+mi}{123}\PYG{p}{)}
\PYG{n}{dml\PYGZus{}plr\PYGZus{}boost} \PYG{o}{=} \PYG{n}{dml}\PYG{o}{.}\PYG{n}{DoubleMLPLR}\PYG{p}{(}\PYG{n}{data\PYGZus{}dml\PYGZus{}base}\PYG{p}{,}
                                \PYG{n}{ml\PYGZus{}g} \PYG{o}{=} \PYG{n}{boost}\PYG{p}{,}
                                \PYG{n}{ml\PYGZus{}m} \PYG{o}{=} \PYG{n}{boost\PYGZus{}class}\PYG{p}{,}
                                \PYG{n}{dml\PYGZus{}procedure}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{dml2}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}
                                \PYG{n}{n\PYGZus{}folds} \PYG{o}{=} \PYG{l+m+mi}{5}\PYG{p}{,}
                                \PYG{n}{n\PYGZus{}rep} \PYG{o}{=} \PYG{l+m+mi}{10}\PYG{p}{)}
\PYG{n}{dml\PYGZus{}plr\PYGZus{}boost}\PYG{o}{.}\PYG{n}{fit}\PYG{p}{(}\PYG{n}{store\PYGZus{}predictions}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{)}
\PYG{n}{boost\PYGZus{}summary5} \PYG{o}{=} \PYG{n}{dml\PYGZus{}plr\PYGZus{}boost}\PYG{o}{.}\PYG{n}{summary}

\PYG{n}{boost\PYGZus{}summary5}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
             coef      std err         t         P\PYGZgt{}|t|        2.5 \PYGZpc{}  \PYGZbs{}
e401  8852.014728  1383.993593  6.395994  1.595063e\PYGZhy{}10  6139.437131   

            97.5 \PYGZpc{}  
e401  11564.592325  
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Regression Decision Trees with 3 folds split}
\PYG{n}{trees} \PYG{o}{=} \PYG{n}{DecisionTreeRegressor}\PYG{p}{(}
    \PYG{n}{max\PYGZus{}depth}\PYG{o}{=}\PYG{l+m+mi}{30}\PYG{p}{,} \PYG{n}{ccp\PYGZus{}alpha}\PYG{o}{=}\PYG{l+m+mf}{0.0047}\PYG{p}{,} \PYG{n}{min\PYGZus{}samples\PYGZus{}split}\PYG{o}{=}\PYG{l+m+mi}{203}\PYG{p}{,} \PYG{n}{min\PYGZus{}samples\PYGZus{}leaf}\PYG{o}{=}\PYG{l+m+mi}{67}\PYG{p}{)}
\PYG{n}{trees\PYGZus{}class} \PYG{o}{=} \PYG{n}{DecisionTreeClassifier}\PYG{p}{(}
    \PYG{n}{max\PYGZus{}depth}\PYG{o}{=}\PYG{l+m+mi}{30}\PYG{p}{,} \PYG{n}{ccp\PYGZus{}alpha}\PYG{o}{=}\PYG{l+m+mf}{0.0042}\PYG{p}{,} \PYG{n}{min\PYGZus{}samples\PYGZus{}split}\PYG{o}{=}\PYG{l+m+mi}{104}\PYG{p}{,} \PYG{n}{min\PYGZus{}samples\PYGZus{}leaf}\PYG{o}{=}\PYG{l+m+mi}{34}\PYG{p}{)}

\PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{seed}\PYG{p}{(}\PYG{l+m+mi}{123}\PYG{p}{)}
\PYG{n}{dml\PYGZus{}plr\PYGZus{}tree} \PYG{o}{=} \PYG{n}{dml}\PYG{o}{.}\PYG{n}{DoubleMLPLR}\PYG{p}{(}\PYG{n}{data\PYGZus{}dml\PYGZus{}base}\PYG{p}{,}
                               \PYG{n}{ml\PYGZus{}g} \PYG{o}{=} \PYG{n}{trees}\PYG{p}{,}
                               \PYG{n}{ml\PYGZus{}m} \PYG{o}{=} \PYG{n}{trees\PYGZus{}class}\PYG{p}{,}
                               \PYG{n}{n\PYGZus{}folds} \PYG{o}{=} \PYG{l+m+mi}{3}\PYG{p}{,} 
                               \PYG{n}{n\PYGZus{}rep} \PYG{o}{=} \PYG{l+m+mi}{10}\PYG{p}{)}
\PYG{n}{dml\PYGZus{}plr\PYGZus{}tree}\PYG{o}{.}\PYG{n}{fit}\PYG{p}{(}\PYG{n}{store\PYGZus{}predictions}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{)}
\PYG{n}{tree\PYGZus{}summary3} \PYG{o}{=} \PYG{n}{dml\PYGZus{}plr\PYGZus{}tree}\PYG{o}{.}\PYG{n}{summary}

\PYG{n}{tree\PYGZus{}summary3}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
             coef      std err         t         P\PYGZgt{}|t|        2.5 \PYGZpc{}  \PYGZbs{}
e401  8494.390142  1332.352929  6.375481  1.823902e\PYGZhy{}10  5883.026386   

            97.5 \PYGZpc{}  
e401  11105.753898  
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Regression Decision Trees with 3 folds split}
\PYG{n}{trees} \PYG{o}{=} \PYG{n}{DecisionTreeRegressor}\PYG{p}{(}
    \PYG{n}{max\PYGZus{}depth}\PYG{o}{=}\PYG{l+m+mi}{30}\PYG{p}{,} \PYG{n}{ccp\PYGZus{}alpha}\PYG{o}{=}\PYG{l+m+mf}{0.0047}\PYG{p}{,} \PYG{n}{min\PYGZus{}samples\PYGZus{}split}\PYG{o}{=}\PYG{l+m+mi}{203}\PYG{p}{,} \PYG{n}{min\PYGZus{}samples\PYGZus{}leaf}\PYG{o}{=}\PYG{l+m+mi}{67}\PYG{p}{)}
\PYG{n}{trees\PYGZus{}class} \PYG{o}{=} \PYG{n}{DecisionTreeClassifier}\PYG{p}{(}
    \PYG{n}{max\PYGZus{}depth}\PYG{o}{=}\PYG{l+m+mi}{30}\PYG{p}{,} \PYG{n}{ccp\PYGZus{}alpha}\PYG{o}{=}\PYG{l+m+mf}{0.0042}\PYG{p}{,} \PYG{n}{min\PYGZus{}samples\PYGZus{}split}\PYG{o}{=}\PYG{l+m+mi}{104}\PYG{p}{,} \PYG{n}{min\PYGZus{}samples\PYGZus{}leaf}\PYG{o}{=}\PYG{l+m+mi}{34}\PYG{p}{)}

\PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{seed}\PYG{p}{(}\PYG{l+m+mi}{123}\PYG{p}{)}
\PYG{n}{dml\PYGZus{}plr\PYGZus{}tree} \PYG{o}{=} \PYG{n}{dml}\PYG{o}{.}\PYG{n}{DoubleMLPLR}\PYG{p}{(}\PYG{n}{data\PYGZus{}dml\PYGZus{}base}\PYG{p}{,}
                               \PYG{n}{ml\PYGZus{}g} \PYG{o}{=} \PYG{n}{trees}\PYG{p}{,}
                               \PYG{n}{ml\PYGZus{}m} \PYG{o}{=} \PYG{n}{trees\PYGZus{}class}\PYG{p}{,}
                               \PYG{n}{n\PYGZus{}folds} \PYG{o}{=} \PYG{l+m+mi}{5}\PYG{p}{,} 
                               \PYG{n}{n\PYGZus{}rep} \PYG{o}{=} \PYG{l+m+mi}{10}\PYG{p}{)}
\PYG{n}{dml\PYGZus{}plr\PYGZus{}tree}\PYG{o}{.}\PYG{n}{fit}\PYG{p}{(}\PYG{n}{store\PYGZus{}predictions}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{)}
\PYG{n}{tree\PYGZus{}summary5} \PYG{o}{=} \PYG{n}{dml\PYGZus{}plr\PYGZus{}tree}\PYG{o}{.}\PYG{n}{summary}

\PYG{n}{tree\PYGZus{}summary5}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
             coef      std err         t         P\PYGZgt{}|t|        2.5 \PYGZpc{}  \PYGZbs{}
e401  8365.634772  1319.168039  6.341599  2.273925e\PYGZhy{}10  5780.112926   

            97.5 \PYGZpc{}  
e401  10951.156619  
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\begin{sphinxuseclass}{cell}
\begin{sphinxuseclass}{tag_hide-input}\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
                coef      std err        2.5 \PYGZpc{}        97.5 \PYGZpc{}
forest   8961.175025  1309.593551  6394.418831  11527.931219
tree     8365.634772  1319.168039  5780.112926  10951.156619
xgboost  8852.014728  1383.993593  6139.437131  11564.592325
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\end{sphinxuseclass}
\sphinxAtStartPar
The summary of orthogonalization, DML with 3 folds, and DML with 5 folds sample splits are shown in the dataframe below. We can see that, as we increase the sample splits, the standard error decrease which gives us a tighter confidence bounds and a more robust ATE estimate

\begin{sphinxuseclass}{cell}
\begin{sphinxuseclass}{tag_hide-input}\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
                                 coef      std err        2.5 \PYGZpc{}        97.5 \PYGZpc{}
Model         ML                                                             
Orthogonal    linear      5896.198421     1249.446     3447.029      8345.367
              polynomial  6084.770503     1247.310     3639.789      8529.752
PLR (3 folds) forest      9018.368261  1315.291812   6440.44368  11596.292842
              tree        8494.390142  1332.352929  5883.026386  11105.753898
              xgboost     9002.744739  1399.883887  6259.022737   11746.46674
PLR (5 folds) forest      8961.175025  1309.593551  6394.418831  11527.931219
              tree        8365.634772  1319.168039  5780.112926  10951.156619
              xgboost     8852.014728  1383.993593  6139.437131  11564.592325
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\end{sphinxuseclass}

\subsection{Summary}
\label{\detokenize{orthogonal_DML:summary}}
\sphinxAtStartPar
Double Machine Learning (DML) leverages predictive power of advance Machine Learning (ML) algrotighms in estimating heterogeneous treatment effects when all potential confounders are observed and are also high\sphinxhyphen{}dimensional. At its core, DML utilizes orthogonalization to address the regularization bias induced by ML algorithm in estimating high\sphinxhyphen{}dimensional nuisance parameters. DML requires two ML methods to predict treatment and outcome using the observed covariates. The residuals from the treatment and outcome model is then used to estimate the causal parameter of interest, the treatment effect. The purpose of the treatment residuals is to represent the debiased version of the treatment model because, by definition, residuals are orthogonal to the features used to contruct the model. Similarly, the outcome residuals denoises the outcome model because the outcome residuals can essentially be viewed as a version of the treatment where all the variance from the features are explained. Thus, DML provides a general yet robust framework for estimating and performing inference on treatment/causal variables.


\subsection{References}
\label{\detokenize{orthogonal_DML:references}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
1. Chernozhukov, V., Chetverikov, D., Demirer, M., Duflo, E., Hansen, C., Newey, W., \PYGZam{} Robins, J. (2018). Double/debiased machine learning for treatment and structural parameters.

2. Glynn, A. N., \PYGZam{} Quinn, K. M. (2010). An introduction to the augmented inverse propensity weighted estimator. Political analysis, 18(1), 36\PYGZhy{}56.

3. https://matheusfacure.github.io/python\PYGZhy{}causality\PYGZhy{}handbook/22\PYGZhy{}Debiased\PYGZhy{}Orthogonal\PYGZhy{}Machine\PYGZhy{}Learning.html

4. https://www.youtube.com/watch?v=eHOjmyoPCFU\PYGZam{}t=444s

5. Bach, P., Chernozhukov, V., Kurz, M. S., and Spindler, M. (2022), DoubleML \PYGZhy{} An Object\PYGZhy{}Oriented Implementation of Double Machine Learning in Python, Journal of Machine Learning Research, 23(53): 1\PYGZhy{}6, https://www.jmlr.org/papers/v23/21\PYGZhy{}0862.html.

6. Frisch, R., \PYGZam{} Waugh, F. V. (1933). Partial time regressions as compared with individual trends. Econometrica: Journal of the Econometric Society, 387\PYGZhy{}401.

7. Lovell, M. C. (1963). Seasonal adjustment of economic time series and multiple regression analysis. Journal of the American Statistical Association, 58(304), 993\PYGZhy{}1010.

8. Lovell, M. C. (2008). A simple proof of the FWL theorem. The Journal of Economic Education, 39(1), 88\PYGZhy{}91.
\end{sphinxVerbatim}

\sphinxstepscope


\chapter{\sphinxstylestrong{Conclusion and New Directions}}
\label{\detokenize{Conclusion:conclusion-and-new-directions}}\label{\detokenize{Conclusion::doc}}

\section{Summary of Estimators}
\label{\detokenize{Conclusion:summary-of-estimators}}\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{AIPW} \sphinxhyphen{} It is a weighting based estimator that improves IPTW by fully utilizing information about both the treatment assignment and the outcome. It is a combination of IPTW and a weighted average of the outcome regression estimators.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{TMLE} \sphinxhyphen{} It incorporates a targeting step that optimizes the bias\sphinxhyphen{}variance tradeoff for the targeted estimator, i.e., ATE. It obtains initial outcome estimates via outcome modeling and propensity scores via treatment modeling, respectively. These initial outcome estimates are then updated to reduce the bias of confounding, which generates the targeted predicted outcome values.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{DML} \sphinxhyphen{} It utilizes predictive power of advanced ML algorithms in estimating heterogeneous treatment effects when all potential confounders are observed and are also high\sphinxhyphen{}dimensional.

\end{itemize}


\section{Doubly Robust Methods: Applications}
\label{\detokenize{Conclusion:doubly-robust-methods-applications}}\begin{itemize}
\item {} 
\sphinxAtStartPar
Molecular Epidemiology \sphinxhyphen{} Meng et al. (2021) applied efficient estimators like AIPW and TMLE to estimate average treatment effects under various scenarios of mis\sphinxhyphen{}specification.

\item {} 
\sphinxAtStartPar
Social Sciences \sphinxhyphen{} Knaus (2020) showed the efficiency of DML for the evaluation of programs of the Swiss Active Labour Market Policy.

\item {} 
\sphinxAtStartPar
Medical Sciences \sphinxhyphen{} Rose et al. (2020) proposed the use of TMLE for the evaluation of the comparative effectiveness of drug\sphinxhyphen{}eluting coronary stents.

\end{itemize}


\section{Potential Future Works (Tan et al. (2022))}
\label{\detokenize{Conclusion:potential-future-works-tan-et-al-2022}}\begin{itemize}
\item {} 
\sphinxAtStartPar
It is recommended to do a variable selection first, followed by using SuperLearner to model PS and outcomes. After that TMLE can be applied for estimating ATE.

\item {} 
\sphinxAtStartPar
The use of ML algorithms like random forest and neural networks can be used to remove treatment predictors for variable selection.

\item {} 
\sphinxAtStartPar
Soft variable selection strategies can be used where the variable selection is conducted without requiring any modeling on the outcome regression, and thus provides robustness against mis\sphinxhyphen{}specification.

\end{itemize}







\renewcommand{\indexname}{Index}
\printindex
\end{document}