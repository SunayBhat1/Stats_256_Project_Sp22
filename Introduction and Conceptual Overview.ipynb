{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba9bf0d4",
   "metadata": {},
   "source": [
    "## What does \"doubly robust\" mean? \n",
    "\n",
    "Doubly robust methods estimate two models:\n",
    "- an *outcome model*\n",
    "$$\\mu_d(X_i) = E(Y_i \\mid D_i = d, X_i)$$\n",
    "- and a *exposure model* (or treament model or propensity score):\n",
    "$$\\pi(X_i) = E(D_i \\mid X_i)$$\n",
    "\n",
    "where $\\mu_d(\\cdot)$ is the model of control or treatment $D_i = d=\\{0, 1\\}$, $X_i$ is a vector of covariates for unit $i = 1, \\ldots, N$, $Y_i$ is the outcome, and $\\pi(\\cdot)$ is the exposure model. Note that covariates included in $X_i$ can be different for the two models. \n",
    "\n",
    "An estimator is called \"doubly robust\" if it achieves constistent estimation of the ATE (or whatever estimand we're interested in) as long as *at least one* of these two models is consistently estimated. This means that the outcome model can be completely misspecified, but as long as the exposure model is correct, our estimation of the ATE will be consistent. This also means that the exposure model can be completely wrong, as along as the outcome model is correct.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c9d6e7b",
   "metadata": {},
   "source": [
    "## Origins of Doubly Robust Methods\n",
    "\n",
    "According to Bang and Robins (2005), doubly robust methods have their origins in missing data models Robins, Rotnitzky, and Zhao (1994) and Rotnitzky, Robins, and Scharfstein (1998) developed augmented orthogonal inverse probability-weighted (AIPW) estimators in missing data models, and Scharfstein, Rotnitzky, and Robins (1999) showed that AIPW was doubly robust and extended to causal inference.  \n",
    "\n",
    "But Kang and Schafer (2007) argue that doubly robust methods are older. They cite work by Cassel, Särndal, and Wretman (1976, 1977), who proposed “generalized regression estimators” for population means from surveys where sampling weights must be estimated.  \n",
    "\n",
    "Arguably, doubly robust methods go back even further than this. The form of doubly robust methods is similar to residual-on-residual regression, which dates back to Frisch, Waugh, and Lovell (1933) famous FWL theorem:\n",
    "$$\\beta_D = \\frac{\\text{Cov}(\\tilde Y_i, \\tilde D_i)}{\\text{Var}(\\tilde D_i)}$$\n",
    "where $\\tilde D_i$ is the residual part of $D_i$ after regressing it on $X_i$, and $\\tilde Y_i$ is the residual part of $Y_i$ after regressing it on $X_i$.  \n",
    "\n",
    "There are also links between doubly robust methods and matching with regression adjustment. This work goes back to at least Rubin (1973), who suggested that regression adjustment in matched data produces less biased estimates that either matching (exposure adjustment) or regression (outcome adjustment) do by themselves. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14583556",
   "metadata": {},
   "source": [
    "## Assumptions\n",
    "\n",
    "Most doubly robust methods require almost all of the standard assumptions necessary for all methods that depend on selection on observables. Although some doubly robust methods relax one or two of these, the six standard assumptions are:\n",
    "1. Consistency\n",
    "2. Positivity/overlap\n",
    "3. One version of treatment\n",
    "4. No interference\n",
    "5. IID observations\n",
    "6. Conditional ignorability: $\\{Y_{i0}, Y_{i1}\\} \\perp \\!\\!\\! \\perp D_i \\mid X_i$\n",
    "\n",
    "Special attention should be paid to Assumption 6: doubly robust methods will not work if we do not measure an important confounder that affects both treatment and exposure. But notably, the doubly robust methods covered in this tutorial make no functional form assumptions. Most use flexible machine learning algorithms to estimate both the outcome and exposure models, with regularization (often through cross-fitting) to avoid overfitting.  \n",
    "\n",
    "If these six assumptions are met, and we use the right estimator, we get double robustness: consistent estimation if either treatment or outcome model correct.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af1ba6be",
   "metadata": {},
   "source": [
    "## A simple demonstration: augmented inverse probability weights\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
